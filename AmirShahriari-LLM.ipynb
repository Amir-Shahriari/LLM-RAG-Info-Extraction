{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c6ebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "In summary, this document presents the results of a test conducted on welder-joint defects with two different types of steel. The test involved one welder who used two different welding machines for each type of steel, and the results were analyzed to determine if there was any significant difference in defect counts across both joints. While this test method is novel and innovative, it also presents some challenges, such as the variability in data and the need for more consistent results before considering fully switching over to this new technique. However, with the help of real-time sensor technology and machine learning algorithms, this technique has shown potential for reducing heat input variations and improving joint defect reduction.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Dummy.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and please include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7ac670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to: Te Augmentation of Jurka T-cells ProlifeR-ation with ccRBCs by Staining with CFSE, Induction by ORLyte and A549 Conditioned Media Stimulating the ProlifeR-ation in Vitro.\n",
      "\n",
      "Augmentation of Jurka T-cells prolifeR-ation with ccRBCs was investigated using the leukemia T-cell line, Jurkat cells. For intact RBCs, ccRBCs had a most significant impact on the prolifeR-ation, leading to an increase in cellular prolifeR-ation of 8.3-fold, 2.4-fold, and 6.1-fold with MFI (mean fold change) values of 3.5/1.7/3.0 respectively compared to the control (Figure 2a). In contrast, A549, oRBCs incubaited alone did not stimulate significant prolifeR-ation, leading to mean fold changes of 2.9/3.5 and 1.5 respectively with MFI values of 2.9/3.5 (Figure 2b). ORLyte alone stimulated the Jurkat cells to increase the prolifeR-ation by 7.6-fold, 4.7-fold, and 6.8-fold compared to the control with MFI values of 10.8/3.1/5.9 (Figure 2b). Furthermore, ccRBCs stimulated Jurka t-cell prolifeR-ation to the highest levels, leading to a mean fold change of 7.6/1.7/4.7 with MFI values of 8.3/2.9/5.9 respectively compared to oRBC conditioned media (Figure 2c). Therefore, A549 and ccRBCs stimulated signifcantlly more prolifeR-ation than oRBC conditioned media in vitro. The findings suggest that RBCs are known ligands of T-cells in the tumour vasculature, leading to cellular prolifeR-ation and a heightened immune response in contrast to the normal circulating blood.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"RedBlood.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.  \n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2db7b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovative Wireless Charger Using Inductive Coupling for Portable Devices: A Study on Device Performance, Battery Life, and Rechargeability\n",
      "\n",
      "Introduction:\n",
      "Wireless chargers have become a popular feature in modern devices such as smartphones, tablets, and laptops. However, these wireless chargers often face compatibility issues with the device being charged, which limits their practical use. To overcome this issue, researchers have proposed innovative solutions using inductive coupling for battery-powered devices. This study investigates device performance, battery life, and rechargeability in such wireless chargers using inductive coupling.\n",
      "\n",
      "Methodology:\n",
      "The following methodology was employed to evaluate the efficiency of inductive charging:\n",
      "\n",
      "1. Design of Experiment (DOE): A 6-factorial DOE design with a 3-level factors for device, charging system, battery type, operating condition (warm, cold), and ambient temperature (45°C) was employed.\n",
      "2. Procedure for Device Selection: The five tested devices are Apple iPhone SE, Samsung Galaxy S9 Plus, Google Pixel 3, LG V30+, and Huawei Mate 10 Pro. All these devices have different battery types (Li-ion/NiMh), operating conditions (warm, cold), and ambient temperatures (45°C).\n",
      "3. Battery Capacity Test: The charging performance of each device was tested in terms of battery capacity using a battery pack. Three devices (iPhone SE, Samsung Galaxy S9 Plus, Google Pixel 3) were charged to their full battery capacities and then tested in terms of rechargeability and efficiency.\n",
      "4. Rechargeability Test: The charging performance was tested by charging the battery at its full capacity and comparing it with the device's current use.\n",
      "5. Battery Life Test: Each battery pack was charged to its full capacity and tested for battery life, which is the average time taken by each device to fully charge.\n",
      "6. Efficiency Test: The charging performance of each device was tested under different ambient temperatures between 20°C and 45°C.\n",
      "7. Compatibility Test: The devices' compatibility with other wireless chargers, such as those with inductive coupling and non-inductive charging, was tested to ensure the devices' use in wireless charging environments is not limited by the device's battery type or operating conditions.\n",
      "\n",
      "Results:\n",
      "The results showed that all devices exhibited excellent performance and rechargeability when using inductive coupling. All devices' batteries were fully charged within 1 hour, which was significantly faster than the conventional charging methods (iPhone SE: 2 hours; Samsung Galaxy S9 Plus: 3 hours; Google Pixel 3: 3 hours). The rechargeability test showed that all devices had a good battery life with an average life of approximately 6 hours, and the iPhone SE exhibited the longest battery life (7.5 hours), which is in line with other smartphone manufacturers' battery life expectations. In terms of efficiency, the devices performed well in all conditions tested, reaching an efficiency of 98% for the Samsung Galaxy S9 Plus and a high efficiency of 99% for the Pixel 3 devices.\n",
      "\n",
      "Conclusion:\n",
      "The results of this study have demonstrated that inductive coupling is a suitable method to charge wireless charging compatible devices using batteries. The proposed devices are highly efficient, with low recharge time, and long battery life. The compatibility test indicates that all devices can be used in wireless chargers with inductive coupling. This technology provides improved performance, reliability, and efficiency compared to conventional charging methods.\n",
      "\n",
      "Conclusion:\n",
      "The results of this study have demonstrated that inductive coupling is a suitable method for wireless charging compatible devices using batteries. The proposed devices are highly efficient, with low recharge time, and long battery life. The compatibility test indicates that all devices can be used in wireless chargers with inductive coupling. This technology provides improved performance, reliability, and efficiency compared to conventional charging methods.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"DOMV.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28862bcc",
   "metadata": {},
   "source": [
    "concise statement about the expected result of an experiment or project. In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996a278",
   "metadata": {},
   "source": [
    "### Date : 2024/09/02\n",
    "#### Time : 19:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dafe50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In conclusion, the device's primary function is to transmit radio frequency signals over long distances to and from an antenna in order to monitor and manage the battery system within a miniNG industry. The device's housing, coupled with a battery charging arrangement located in the void and electrically coupled to the battery, enables accurate and efficient data transmission. The device also comprises a battery, inductive coupling, radio frequency transmitter, receiver, microcontroller, RF transmitter/receiver, on/off switch, and a baseplate with threaded member for fitting to a miniNG base plate. The device's design is suitable for harsh industrial environments, including those found in the miniNG industry.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"DOMV.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "Extract the hypothesis related\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and please include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3e0d7",
   "metadata": {},
   "source": [
    "### Date : 2024/09/04\n",
    "#### Time : 21:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a7f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Hypothesis: The study examined the impact of multiple stressors on cellular phenotypes in two types of cancer cells, namely acute myeloid leukemia (AML) and hepatocellular carcinoma (HCC), as well as their responses to therapy. The researchers found that when cells are subjected to multiple stressors simultaneously, they have increased vulnerability and resistance to cell death, decreased viability, and altered gene expression patterns, all of which can contribute to the development and progression of cancer.\n",
      "\n",
      "More specifically, the authors found that in both AML and HCC cells subjected to a combination of hypoxia, cytokine-secretion activity, allo-stimulation, and microRNA (miR) content, they experienced alterations in cell cycle regulation, proliferation, migration, and invasion. They also observed changes in the expression of genes involved in inflammation, DNA repair, apoptosis, and signal transduction.\n",
      "\n",
      "The findings suggest that multiple stressors can differentially impact cancer cells' response to therapy and lead to a range of adverse outcomes, including increased sensitivity to chemotherapeutics and enhanced resistance to immunotherapies. The study highlights the importance of studying multiple stressors in cancer cell biology and their implications for the development and treatment of cancer.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Dummy.txt\"\n",
    "example_hypothesis_path = \"Dummy_Hypothesis.txt\"\n",
    "new_text_path = \"RedBlood.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "Now, given the following text, extract the hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b6d37",
   "metadata": {},
   "source": [
    "### Date : 2024/09/10\n",
    "#### Time : 20:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c9d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the device as claimed in claim 7 of the patent application meets all requirements for a viability sensor. The housing has an inner surface defined by a void that receives a coupling arrangement and extends outwardly from a main surface of the cover, which has a threaded section for receiving a correspondingly threaded member fixed to the baseplate. The coupling arrangement is located in the void, allowing it to be removably fitted to the housing. The viability sensor is coupled to the coupling arrangement and electrically coupled to the electronic circuit located in the void. The electronic circuit is arranged to receive a signal from the sensor and process the signal to create viability data. The device also includes a battery, an on/off switch, and a plurality of protusions for fixing the device to a baseplate.\n",
      "\n",
      "The device as claimed in any one of the preceding claims meets all requirements for a viability sensor. Its housing has a void that receives a coupling arrangement and extends outwardly from a main surface of the cover, allowing it to be removably fitted to the housing. The electronic circuit is located in the void, allowing it to receive a signal from the sensor and process the signal to create viability data. The device also includes a battery, an on/off switch, and a plurality of protusions for fixing the device to a baseplate.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "Definition of Hypothesis: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "Now, given the following text, extract the hypothesis, remember to label the extracted chunk as Hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49aee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: So, how did you ensure that everything else was kept the same while welding steel plates?\n",
      "\n",
      "Person 2: We used 10mm thick steel plate and made sure it was high-grade.\n",
      "\n",
      "Person 3: What is the Welder's Inspector, and what kind of inspection tools do they use to ensure consistent results?\n",
      "\n",
      "Person 1: The Welder's Inspector checks that the welder's tools are calibrated correctly and that there is no deviation from the established pattern. They also monitor the welder's performance during the process, ensuring consistency in the number of defects per joint.\n",
      "\n",
      "Person 2: We used ultrasonic testing equipment to ensure that the steel plates were not contaminated with other metals or impurities, and we also checked for small inconsistencies. But even then, there were still a few outliers, and it's hard to say which ones are due to defective material and which are due to human error.\n",
      "\n",
      "Person 3: The Welder's Inspector uses statistical significance to determine if the new technique's results are consistent with what we saw before. While we did a t-test, there is still a margin of error, and we need more data to make a firm conclusion.\n",
      "\n",
      "Person 1: So, it looks like we'll have to wait for more data to determine if this new welding technique is truly worth implementing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "new_text_path = \"Data/Dummy.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Definition of Hypothesis: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "\n",
    "\n",
    "Now, given the following text, extract the hypothesis, remember to label the extracted chunk as Hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64862ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embodiement of a device for collecting vibraction data comprises a housing with an inner surface defining a void, a coupling arrangement for coupling the housing to a baseplate, a vibraction sensor located in the void, and an electronic circuit arranged to receive signals from the sensor and process them to create vibraction data. The device is formed from nylon polyamide and IP68 standards, allowing it to be suitable for use in harsh environments.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load and prepare document\n",
    "document_content = load_text_file(\"Data/device.txt\")\n",
    "\n",
    "# Split into sentences for more precise chunking\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "chunks = sent_tokenize(document_content)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Define query with a refined approach\n",
    "query = \"\"\"\n",
    "Please extract and clearly label the **Hypothesis** from the document content. \n",
    "A hypothesis is a statement that proposes a potential explanation for a phenomenon or a specific outcome of an experiment. \n",
    "It is a concise and testable prediction.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Hypothesis:\n",
    "1. [Extracted hypothesis here]\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve relevant chunks and prepare prompt\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Given the following relevant text:\n",
    "    {relevant_text}\n",
    "\n",
    "    Extract the hypothesis as defined below:\n",
    "    A research hypothesis is a statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "    **Hypothesis:**\n",
    "    1. [Extracted hypothesis here]\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6ba96",
   "metadata": {},
   "source": [
    "### Date : 2024/09/12\n",
    "#### Time : 11:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8b0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment supports the hypothesis that regular aerobic exercise (e.g., running or cycling) enhances cognitive function, particularly in memory and executive control tasks, in adults aged 25-50. The improvements observed are evident in memory-related tasks and tasks requiring executive control, such as planning and problem-solving, which can be attributed to the positive effects of physical activity on brain health. These findings provide evidence that regular aerobic exercise is an effective intervention for enhancing cognitive function in adults, which could have significant implications for daily life and professional domains.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"\n",
    "Please extract and clearly label the **Hypothesis** from the document content. \n",
    "A hypothesis is a statement that proposes a potential explanation for a phenomenon or a specific outcome of an experiment. \n",
    "It is a concise and testable prediction.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Hypothesis:\n",
    "1. [Extracted hypothesis here]\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "    Now, given the following relevant text, extract the hypothesis:\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Hypothesis:**\n",
    "    1. [Extracted hypothesis here]\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5bb81",
   "metadata": {},
   "source": [
    "### Date : 2024/09/13\n",
    "#### Time : 12:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c692272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group.\n",
      "The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions.\n",
      "Both groups will undergo the same curriculum but with different learning supports.\n",
      "Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Both groups will undergo the same curriculum but with different learning supports. Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "\n",
      "Filled Prompt:\n",
      "\n",
      "    Here is an example of a text with its corresponding hypothesis:\n",
      "\n",
      "    **Example Text:**\n",
      "    Participant Selection:\n",
      "\n",
      "Recruit 100 adults aged 25-50.\n",
      "Split the participants into two groups:\n",
      "Experimental group: Engages in 30 minutes of aerobic exercise (e.g., running or cycling) 5 times a week for 12 weeks.\n",
      "Control group: No regular exercise during the same period.\n",
      "Cognitive Function Tests:\n",
      "\n",
      "Measure participants' baseline cognitive abilities using a standardized battery of cognitive tests (focusing on memory recall, problem-solving, and executive control).\n",
      "Administer the same tests at intervals: before starting the regimen, at 6 weeks, and at the end of 12 weeks.\n",
      "Physiological Measurements:\n",
      "\n",
      "Track physical health improvements (heart rate, VO2 max) in the exercise group to ensure that aerobic capacity is improving alongside cognitive measures.\n",
      "\n",
      "\n",
      "Regular aerobic exercise enhances cognitive function, especially in tasks involving memory and executive control, in adults between the ages of 25 and 50.\n",
      "\n",
      "Cognitive Test Scores:\n",
      "\n",
      "Over the 12-week period, participants in the experimental group show an improvement in memory recall and executive function tasks compared to their baseline scores.\n",
      "The control group shows little to no improvement or a slight decline in cognitive function over time.\n",
      "Physiological Improvement:\n",
      "\n",
      "The experimental group shows increased aerobic capacity (measured through VO2 max) and lower resting heart rates, indicating enhanced physical fitness.\n",
      "Qualitative Feedback:\n",
      "\n",
      "Some participants in the experimental group report feeling more alert and focused in daily tasks, while control group participants report no significant changes.\n",
      "\n",
      "The experimental group exhibits a statistically significant improvement in cognitive function compared to the control group. The improvements are particularly noticeable in memory-related tasks and tasks requiring executive control (like planning and problem-solving).\n",
      "The correlation between improved physical fitness and cognitive enhancement supports the idea that aerobic exercise boosts brain function.\n",
      "\n",
      "The experiment supports the hypothesis that regular aerobic exercise improves cognitive function in adults aged 25-50. The increase in cognitive abilities, particularly in memory and executive control, can be attributed to the positive effects of physical activity on brain health. Hence, aerobic exercise should be considered a beneficial intervention for enhancing cognitive performance in adults.\n",
      "    \n",
      "    **Example Hypothesis:**\n",
      "    Engaging in regular aerobic exercise improves cognitive function, particularly in tasks related to memory and executive control, in adults aged 25-50.\n",
      "\n",
      "\n",
      "    \n",
      "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
      "    \n",
      "    **Definition of Hypothesis:**\n",
      "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
      "\n",
      "    **Relevant Text:**\n",
      "    Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Both groups will undergo the same curriculum but with different learning supports. Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "\n",
      "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
      "    Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Both groups will undergo the same curriculum but with different learning supports. Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "    \n",
      "    **Hypothesis:**\n",
      "    \n",
      "The provided example hypothesis is derived from the relevant text, specifically, over the course of a semester, the Artificial Intelligence (AI) group demonstrates a marked improvement in test scores compared to the control group. The AI group receives personalized learning recommendation based on their performance, learning style, and pace, while the control group follows standard instruction without personalized suggestions. This suggests that by utilizing technology to provide more targeted and effective learning supports, students in the AI group may demonstrate higher levels of interest and motivation for the same curriculum and instruction as those in the control group.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"****\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/Gen2Doc.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "print(\"\\nFilled Prompt:\")\n",
    "print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a95e8",
   "metadata": {},
   "source": [
    "### Date : 2024/09/19\n",
    "#### Time : 20:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484be158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'name': 'flanT5:latest', 'model': 'flanT5:latest', 'modified_at': '2024-10-18T09:30:47.189735154Z', 'size': 496288729, 'digest': '7a929ef5596adbc4f8a090da61da9c23e02c8cacce0c547059ddc9765cda8eb3', 'details': {'parent_model': '', 'format': 'gguf', 'family': 't5', 'families': ['t5'], 'parameter_size': '247.58M', 'quantization_level': 'F16'}}, {'name': 'flanT5_Q8:latest', 'model': 'flanT5_Q8:latest', 'modified_at': '2024-10-18T01:48:56.267189427Z', 'size': 310495610, 'digest': 'f3d524247f7e302770f085c1e52f0b913ab336d5a1c75edcafe654b3b8625120', 'details': {'parent_model': '', 'format': 'gguf', 'family': 't5', 'families': ['t5'], 'parameter_size': '247.58M', 'quantization_level': 'Q8_0'}}, {'name': 'phi3.5:latest', 'model': 'phi3.5:latest', 'modified_at': '2024-10-18T01:41:23.843218919Z', 'size': 2176178843, 'digest': '61819fb370a3c1a9be6694869331e5f85f867a079e9271d66cb223acb81d04ba', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '3.8B', 'quantization_level': 'Q4_0'}}, {'name': 'phi:latest', 'model': 'phi:latest', 'modified_at': '2024-10-18T01:40:36.235231383Z', 'size': 1602463378, 'digest': 'e2fd6321a5fe6bb3ac8a4e6f1cf04477fd2dea2924cf53237a995387e152ee9c', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi2', 'families': ['phi2'], 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma2:2b', 'model': 'gemma2:2b', 'modified_at': '2024-10-18T01:39:44.799247654Z', 'size': 1629518495, 'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma2', 'families': ['gemma2'], 'parameter_size': '2.6B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma:2b', 'model': 'gemma:2b', 'modified_at': '2024-10-18T01:39:10.735260179Z', 'size': 1678456656, 'digest': 'b50d6c999e592ae4f79acae23b4feaefbdfceaa7cd366df2610e3072c052a160', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma', 'families': ['gemma'], 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'stablelm2:latest', 'model': 'stablelm2:latest', 'modified_at': '2024-10-18T01:38:33.107275771Z', 'size': 982790462, 'digest': '714a6116cffa8b415b52c62a7a2d09ba6227ed733baa0025c937a36aee5504f3', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'stablelm', 'families': ['stablelm'], 'parameter_size': '2B', 'quantization_level': 'Q4_0'}}, {'name': 'orca-mini:latest', 'model': 'orca-mini:latest', 'modified_at': '2024-10-18T01:38:06.511287973Z', 'size': 1979947443, 'digest': '2dbd9f439647093cf773c325b0b3081a11f1b1426d61dee8b946f8f6555a1755', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': None, 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'tinydolphin:latest', 'model': 'tinydolphin:latest', 'modified_at': '2024-10-18T01:37:23.151310125Z', 'size': 636743607, 'digest': '0f9dd11f824c7f9a881c9e663d71c1bb0ed0b1d76dd21f6c679f7193c3be7308', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '1B', 'quantization_level': 'Q4_0'}}, {'name': 'tinyllama:latest', 'model': 'tinyllama:latest', 'modified_at': '2024-10-18T01:37:01.427322338Z', 'size': 637700138, 'digest': '2644915ede352ea7bdfaff0bfac0be74c719d5d5202acb63a6fb095b52f394a4', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '1B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:1.5b', 'model': 'qwen2:1.5b', 'modified_at': '2024-10-18T01:36:38.611336019Z', 'size': 934964102, 'digest': 'f6daf2b25194025ae2d5288f2afd041997ce48116807a3b612c1a96b09bec03a', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '1.5B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen:1.8b', 'model': 'qwen:1.8b', 'modified_at': '2024-10-18T01:36:17.071349768Z', 'size': 1120243281, 'digest': 'b6e8ec2e7126ea21d1817e28ad69a2bebdd5547a9af223fbb927054dc66fc4ce', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '2B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:0.5b', 'model': 'qwen2:0.5b', 'modified_at': '2024-10-18T01:36:17.955349187Z', 'size': 352164041, 'digest': '6f48b936a09f7743c7dd30e72fdb14cba296bc5861902e4d0c387e8fb5050b39', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '494.03M', 'quantization_level': 'Q4_0'}}, {'name': 'qwen:0.5b', 'model': 'qwen:0.5b', 'modified_at': '2024-10-18T01:35:52.631366389Z', 'size': 394998579, 'digest': 'b5dc5e784f2a3ee1582373093acf69a2f4e2ac1710b253a001712b86a61f88bb', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '620M', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2.5:0.5b', 'model': 'qwen2.5:0.5b', 'modified_at': '2024-10-17T06:40:37.514909596Z', 'size': 397821319, 'digest': 'a8b0c51577010a279d933d14c2a8ab4b268079d44c5c8830c0a93900f1827c67', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '494.03M', 'quantization_level': 'Q4_K_M'}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Hypothesis:** AI-driven personalized learning tools significantly improve student engagement and academic performance in high school mathematics.\n",
      "\n",
      "**Experiment:** \n",
      "1. The experiment involves 200 high school students split into two groups, one using AI-driven personalized learning tools (AI group) and the other following traditional methods (control group).\n",
      "2. Both groups complete the same math curriculum over a semester.\n",
      "3. Academic performance is measured with tests before and after.\n",
      "\n",
      "**Observation:**\n",
      "1. The AI group demonstrates an improvement in test scores compared to the control group, as evidenced by higher engagement surveys and more active participation in class discussions among students using AI tools.\n",
      "2. Students in the AI group participate more actively in discussions and complete assignments with greater enthusiasm during the semester, which is consistent with their improved academic performance.\n",
      "\n",
      "**Evaluation:**\n",
      "1. The observed data supports the hypothesis that AI-driven personalized learning enhances both engagement and academic performance in high school mathematics by demonstrating statistically significant improvements in test scores and increased motivation among students using AI tools.\n",
      "2. This evidence suggests that incorporating AI into educational settings can lead to better outcomes for students, with potential applications across multiple subject areas.\n",
      "\n",
      "**Logical Conclusions:**\n",
      "1. The results indicate that AI-driven personalized learning improves academic outcomes and student engagement compared to traditional methods.\n",
      "2. These findings validate the effectiveness of AI in enhancing educational experiences and should be considered in broader contexts like teaching and learning practices.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Use list to see available models\n",
    "print(client.list())\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    The following is an example of a text with its corresponding elements: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    **Example Observation:**\n",
    "    {example_observation}\n",
    "    \n",
    "    **Example Evaluation:**\n",
    "    {example_evaluation}\n",
    "    \n",
    "    **Example Logical Conclusions:**\n",
    "    {example_conclusion}\n",
    "    \n",
    "    **Definitions**:\n",
    "    - **Hypothesis**: A concise statement about the expected result of an experiment or project.\n",
    "    - **Experiment**: A structured and controlled procedure to test the hypothesis.\n",
    "    - **Observation**: Data or information collected during the experiment.\n",
    "    - **Evaluation**: An assessment of the results, discussing whether they support the hypothesis.\n",
    "    - **Logical Conclusions**: Deductions based on the evaluation and analysis, forming the final interpretation.\n",
    "\n",
    "    Now, please read the following **Relevant Text** and extract the hypothesis, experiment, observation, evaluation, and logical conclusions:\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Step-by-step reasoning**:\n",
    "    1. Identify statements that propose a relationship or expected result (hypothesis).\n",
    "    2. Identify statements that explain testing or validation steps (experiment).\n",
    "    3. Look for data or information collected during the experiment (observation).\n",
    "    4. Assess statements analyzing the findings (evaluation).\n",
    "    5. Identify conclusions drawn based on evaluation (logical conclusions).\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Describe the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a detailed experimental procedure in 2-3 sentences.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment in 1-2 sentences.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation in 1-2 sentences, discussing support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the logical conclusion in 1-2 sentences.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8208654",
   "metadata": {},
   "source": [
    "### Date : 2024/09/23\n",
    "#### Time : 15:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49d1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text (Hypothesis)**:\n",
      "6, USA). Statistical analysis. Comparison of multiple treatment groups were statistically evaluated using a one-way ANOVA with correction for multiple comparisons to assess statistical signifcance.\n",
      "\n",
      "The hypothesis can be summarized as follows: Statistical analysis was performed using a one-way ANOVA to compare multiple treatment groups and evaluate their significance in statistical terms.\n",
      "\n",
      "**Experiment (Evaluation)**:\n",
      "Data from the experiment were collected, analyzed, and evaluated statistically. A sample of PBMCs was used for fow cytometry analysis. Results indicated that both the PHA-P and PHA+ccRBC treated groups differed significantly from PBMCs at multiple points (p < 0.05, **p < 0.01, or ***p < 0.001). These findings suggest potential benefits of specific treatments on certain populations.\n",
      "\n",
      "**Logical Conclusions (Evaluation)**:\n",
      "The evaluation confirms that the experimental results demonstrate significant differences between treatment groups and the observed statistical significance supports the hypothesis that one-way ANOVA can be used to evaluate statistical comparisons in multiple treatment groups.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the following **Relevant Text**: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Definitions** (simplified):\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Instructions**:\n",
    "    1. Use only the information in the Relevant Text to create responses.\n",
    "    2. Each answer should be concise and based solely on the content provided.\n",
    "    3. Avoid mirroring any example phrases directly; describe observations, evaluations, and conclusions uniquely.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989eca",
   "metadata": {},
   "source": [
    "### Date : 2024/09/27\n",
    "#### Time : 19:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88134793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "Sure! Let's go through each element in the provided text:\n",
      "\n",
      "**Hypothesis:** Identify the hypothesis in 1-2 concise sentences.\n",
      "\n",
      "**Experiment:** Provide a short description of the experiment procedure.\n",
      "\n",
      "**Observation:** Summarize key observations from the experiment.\n",
      "\n",
      "**Evaluation:** Summarize the evaluation, including support for the hypothesis.\n",
      "\n",
      "**Logical Conclusions:** State the final interpretation based on the evaluation.\n",
      "\n",
      "Here's how we can extract these elements:\n",
      "\n",
      "1. **Hypothesis:**\n",
      "   - The Hypothesis provides a clear statement about what is to be identified in the text.\n",
      "   \n",
      "2. **Experiment:**\n",
      "   - The Experiment section describes the specific steps or processes that were followed during the experiment.\n",
      "\n",
      "3. **Observation:**\n",
      "   - Observations are described in detail, providing concrete details of the data gathered.\n",
      "\n",
      "4. **Evaluation:**\n",
      "   - Evaluation summarizes the overall results and how well they support the hypothesis.\n",
      "   \n",
      "5. **Logical Conclusions:**\n",
      "   - The Logical Conclusions draw a conclusion based on the evaluation of the observations.\n",
      "\n",
      "So, the extraction process is:\n",
      "\n",
      "1. Identify the hypothesis in 1-2 concise sentences.\n",
      "2. Provide a short description of the experiment procedure.\n",
      "3. Summarize key observations from the experiment.\n",
      "4. Summarize the evaluation, including support for the hypothesis.\n",
      "5. State the final interpretation based on the evaluation.\n",
      "\n",
      "I'll now apply this process to the given text:\n",
      "\n",
      "**Hypothesis:**\n",
      "- Identify the hypothesis in 1-2 concise sentences.\n",
      "\n",
      "The provided text does not include a clear hypothesis or any specific steps described in an experiment procedure, so I will skip that part and focus solely on the observations and evaluations mentioned.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the text without printing the **Relevant Text** itself. Make sure to address each element (Hypothesis, Experiment, Observation, Evaluation, and Logical Conclusions) based on the entire provided text context.\n",
    "\n",
    "    **Example Definitions**:\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Instructions**:\n",
    "    - Use the entire text context to provide responses for all elements.\n",
    "    - Each answer should be unique and focus only on the extracted content, not examples or phrasing.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \n",
    "    make sure to extract these elements from the relevant text.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7f6af",
   "metadata": {},
   "source": [
    "### Date : 2024/10/05\n",
    "#### Time : 11:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c8acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "1. 2 sentences.\n",
      "2. Procedure: A detailed set of steps or procedures that guide the task.\n",
      "3. Observation: An observation made during the execution of the procedure to assess its outcomes.\n",
      "4. Evaluation: A summary of the results obtained from the observed process and their implications on the hypothesis.\n",
      "5. Logical Conclusions: A final interpretation based on the evaluation, suggesting what can be concluded or drawn from the findings.\n",
      "\n",
      "Note that in this context, \"Relevant Text\" is hidden; you would need to provide a specific text for it to proceed with analysis. Please provide a relevant piece of text for extraction.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with stricter instructions to avoid repetition of instructions\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, extract each element from the entire **Relevant Text** without printing or rephrasing the definitions or instructions.\n",
    "\n",
    "    **Relevant Text is hidden** - Use only the provided context in the Relevant Text to respond directly.\n",
    "\n",
    "    **Directly output the following extracted elements based on the relevant text**:\n",
    "    \n",
    "    - **Hypothesis:** 1-2 sentences.\n",
    "    - **Experiment:** Short description of the procedure.\n",
    "    - **Observation:** Summary of key observations.\n",
    "    - **Evaluation:** Summary of results and support for hypothesis.\n",
    "    - **Logical Conclusions:** Final interpretation based on evaluation.\n",
    "    \n",
    "    Do not include any explanatory or instructional text, definitions, or reiterations. Only provide the extracted content.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84268d04",
   "metadata": {},
   "source": [
    "### Date : 2024/10/11\n",
    "#### Time : 10:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c270a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "- **Hypothesis:** The hypothesis states that we are interested in understanding the effects of exposure to a certain substance on various physiological processes and outcomes.\n",
      "\n",
      "- **Experiment:** An experiment is conducted by measuring the effect of exposing participants to different concentrations of the substance being studied. These measurements can then be analyzed to identify patterns or correlations between exposure levels and observed outcomes.\n",
      "\n",
      "- **Observation:** Observations are made during the experiment phase, where researchers record data points from both the treated group (the subjects exposed to the substance) and a control group that receives no intervention. This information is used to infer potential effects of the substance on different physiological processes.\n",
      "\n",
      "- **Evaluation:** The evaluation stage involves comparing the observed outcomes with expected values through statistical analysis or other methods, to determine if the hypothesis can be supported or contradicted based on the data collected during the experiment.\n",
      "\n",
      "- **Logical Conclusions:** Logical conclusions are drawn from the results of the experiment. These conclusions are derived from the experimental design and interpretation of the data obtained. For example, if it is found that the substance has a significant positive impact on one physiological process but not another due to differences in the study's population or variables considered, then these findings could be used as evidence supporting the hypothesis that exposure to this substance leads to specific effects on different bodily functions.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Create a prompt that requires direct extraction with no additional context or instruction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Extract the following elements from the provided text:\n",
    "\n",
    "    - **Hypothesis**\n",
    "    - **Experiment**\n",
    "    - **Observation**\n",
    "    - **Evaluation**\n",
    "    - **Logical Conclusions**\n",
    "\n",
    "    Directly provide the content for each element.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ae661",
   "metadata": {},
   "source": [
    "### Date : 2024/10/18\n",
    "#### Time : 15:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8d157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Hypothesis**: I will perform an experiment to measure the impact of environmental changes on plant growth.\n",
      "\n",
      "- **Experiment**: I will grow a variety of plants in controlled environments with different levels of environmental changes, such as temperature fluctuations or increased water availability.\n",
      "\n",
      "- **Observation**: After several weeks, I will measure the height and overall health of each plant. I will record the changes in soil moisture level and nutrient content.\n",
      "\n",
      "- **Evaluation**: Based on the observations, I will compare the growth patterns of plants with varying environmental conditions to determine which variables have a greater impact on plant growth.\n",
      "\n",
      "- **Logical Conclusions**: I expect that plants grown under increased water availability will grow taller than those under controlled environments with lower water levels. Additionally, I predict that plants growing in areas with higher nutrient content may outgrow their peers due to better nutrition and increased root growth.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Create a prompt that requires direct extraction with no additional context or instruction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Extract the following elements from the provided text:\n",
    "\n",
    "    - **Hypothesis**\n",
    "    - **Experiment**\n",
    "    - **Observation**\n",
    "    - **Evaluation**\n",
    "    - **Logical Conclusions**\n",
    "\n",
    "    Directly provide the content for each element.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63608c",
   "metadata": {},
   "source": [
    "### Date : 2024/10/26\n",
    "#### Time : 12:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2c3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Hypothesis:**\n",
      "The use of AI to provide personalized learning is expected to increase student engagement, improve academic performance.\n",
      "\n",
      "**Experiment:**\n",
      "The experiment involves 200 students split into two groups: one using AI-driven tools and another using traditional methods. Both groups are given pre- and post-tests on their comprehension of a specific topic.\n",
      "\n",
      "**Observation:**\n",
      "Students using AI tools showed increased engagement in class discussions and enthusiasm in assignments, which were measured by pre-test and post-test scores.\n",
      "\n",
      "**Evaluation:**\n",
      "The data supports the hypothesis that personalized learning through AI improves student performance. Specifically:\n",
      "- The pre-test showed better comprehension than before, indicating a positive impact on students' ability to understand the material.\n",
      "- The post-tests indicated an increase in engagement levels among students using AI tools compared to those using traditional methods.\n",
      "\n",
      "**Logical Conclusions:**\n",
      "The findings of this experiment suggest that AI-driven personalized learning can significantly enhance student learning outcomes across various educational contexts.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Create a prompt that includes brief examples for each element\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Extract the following elements from the provided text based on the examples given:\n",
    "\n",
    "    **Example Hypothesis:** Using AI to provide personalized learning increases student engagement and improves academic performance.\n",
    "    \n",
    "    **Example Experiment:** The experiment involves 200 students split into groups with AI-driven tools versus traditional methods, with results measured by pre- and post-tests.\n",
    "    \n",
    "    **Example Observation:** Students using AI tools showed increased engagement in class discussions and enthusiasm in assignments.\n",
    "    \n",
    "    **Example Evaluation:** The data supports the hypothesis, showing statistically significant improvements in test scores and motivation in the AI group.\n",
    "    \n",
    "    **Example Logical Conclusions:** AI-driven personalized learning appears to enhance learning outcomes, suggesting benefits across various educational contexts.\n",
    "\n",
    "    **Please Extract the Following Elements from the Relevant Text:**\n",
    "\n",
    "    - **Hypothesis:** Describe the main hypothesis.\n",
    "    - **Experiment:** Summarize the experimental setup.\n",
    "    - **Observation:** Outline the key observations.\n",
    "    - **Evaluation:** Summarize the evaluation and its support for the hypothesis.\n",
    "    - **Logical Conclusions:** State the final interpretation based on the findings.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54b3f8",
   "metadata": {},
   "source": [
    "### Date : 2024/10/31\n",
    "#### Time : 11:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c9c217",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text:**\n",
      "\n",
      "1. Hypothesis:\n",
      "   The text does not contain any specific hypothesis.\n",
      "\n",
      "2. Experiment:\n",
      "   - **Procedure**: 6, USA). Statistical analysis. Comparison of multiple treatment groups were statistically evaluated using a one-way ANOVA with correction for multiple comparisons to assess statistical signifcance.\n",
      "   - **Methods**:\n",
      "     * Data was collected from PBMCs (Blood Cells Microscopic Examination) before and after treatment using fow cytometry.\n",
      "     * The analysis method used was one-way ANOVA with correction for multiple comparisons. To evaluate statistical significance, a p-value of < 0.05 was required.\n",
      "\n",
      "3. Observation:\n",
      "   - **Data presented**: Box plots with median concentrations were shown to illustrate the distribution of data values.\n",
      "   - **Observations**:\n",
      "     * All data points are statistically different from PBMCs treated with PHA-P if *p < 0.05, **p < 0.01, ***p < 0.001, or ****p < 0.0001 and PBMC + PHA + oRBCs are statistically different from PBMC + PHA + ccRBCs if #p < 0.05.\n",
      "     * Data points from the group treated with OMEGACOQ (one million of chloramphenicol) were more similar to those in the group treated with PBS (without antibiotics) than those in the group treated with BCG (Bacillus Calmette-Guérin), indicating a significant difference.\n",
      "\n",
      "4. Evaluation:\n",
      "   - **Statistical analysis**: The statistical methods used were appropriate for evaluating multiple treatment groups and comparing them to each other.\n",
      "   - **Multiple comparison correction**: Holm's method was correctly applied, ensuring the significance of individual comparisons within multiple groups is not affected by multiple testing.\n",
      "\n",
      "5. Logical conclusions:\n",
      "   - Based on the evaluation, there is a significant difference in data between different treatment groups (PHA-P vs. OMEGACOQ) and between PBMCs (PBMCs treated with PBS vs. PBMCs treated with BCG).\n",
      "   - The results are consistent with previous studies showing that the use of antibiotics can reduce inflammation, indicating that OMEGACOQ may have a similar effect.\n",
      "   - The observed differences in data suggest potential applications for this treatment regimen.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the following **Relevant Text**: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Definitions** (simplified):\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Instructions**:\n",
    "    1. Use only the information in the Relevant Text to create responses.\n",
    "    2. Each answer should be concise and based solely on the content provided.\n",
    "    3. Avoid mirroring any example phrases directly; describe observations, evaluations, and conclusions uniquely.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a49b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text Hypothesis (50 words):**\n",
      "\n",
      "In the mining industry, vibration data is crucial for monitoring ground vibrations caused by controlled detonations in mines. The ability to monitor these vibrations can be useful for regulatory compliance but also for various non-regulatory applications.\n",
      "\n",
      "**Experiment (150-200 words):**\n",
      "1. **Procedure:**\n",
      "   - Use a cover arranged to fit the housing and allow it to be easily removed.\n",
      "   - Install a radio receiver for receiving a radio signal encoded with program data, which is then processed by the electronic circuit to operate according to the provided program data.\n",
      "\n",
      "2. **Observation (50-100 words):**\n",
      "   - The device successfully receives and processes the encoded program data transmitted via the communication channel.\n",
      "   - After processing the program data, it can be used to control the electronic circuit for collecting vibration data at a specified sample rate, recording only specific times or only collecting vibrations if they exceed certain thresholds.\n",
      "\n",
      "3. **Evaluation (50-100 words):**\n",
      "   - The device operates effectively and efficiently, enabling the collection of vibration data from controlled detonations.\n",
      "   - This demonstrates that the proposed electronic circuit successfully processes the program data to gather vibration data without compromising its functionality.\n",
      "\n",
      "4. **Logical Conclusions (20-30 words):**\n",
      "   - Since the program data is processed and used to control the device, it implies that the system can be effectively designed for both regulatory compliance and non-regulatory applications.\n",
      "   - The ability to monitor and analyze vibrations in mines or tunnels could have significant implications for safety, efficiency, and environmental protection.\n",
      "\n",
      "**Hypothesis (50 words):**\n",
      "\n",
      "A device for monitoring vibrations from controlled detonations in mines is advantageous as it allows the temporal elements of the vibrations to be studied. This information can be particularly beneficial when studying resultant vibrations relative to time.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='****')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the following **Relevant Text**: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Definitions** (simplified):\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Instructions**:\n",
    "    1. Use only the information in the Relevant Text to create responses.\n",
    "    2. Each answer should be concise and based solely on the content provided.\n",
    "    3. Avoid mirroring any example phrases directly; describe observations, evaluations, and conclusions uniquely.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
