{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015976fb-39fe-4625-9290-d6e523de5eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: Good evening, everyone. I'm thrilled to introduce our latest innovation in welding technology—the ultra-precise and robotic Technique B.\n",
      "\n",
      "Person 2: Technique B is a new welder that leverages advanced techniques for cutting-edge precision and efficiency. With its dynamically controlled arc length, it can weld materials with thermal properties that are difficult to achieve through traditional approaches.\n",
      "\n",
      "Person 1: To understand how this cutting-edge approach works, let's take a closer look at Technique B's innovative system.\n",
      "\n",
      "Person 2: Technique B utilizes infrared sensors and machine learning algorithms to monitor temperature variations in the welder. By adjusting the arc length based on these changes, Technique B can reduce heat input and minimize thermal noise, resulting in more consistent welds.\n",
      "\n",
      "Person 1: And here's a comparison between Technique A and B.\n",
      "\n",
      "Person 2: Technique A is a traditional welder that uses manual settings for arc length and temperature. Here are the results:\n",
      "\n",
      "Person 3: Technique A had around 50 defects across 50 joints, averaging 1.5 defects per joint. That’s not a very good figure.\n",
      "\n",
      "Person 2: The system was still not perfect, but we were able to make some improvements. With Technique B, the number of defects is reduced by around 62.5%. But this still isn't perfect. We need more data to know for sure if it's worth replacing the manual welder with.\n",
      "\n",
      "Person 1: That’s where the innovative system comes in. It takes real-time sensor readings and adjusts the arc length accordingly, reducing heat input variations while maintaining consistent welds.\n",
      "\n",
      "Person 2: The result is a significant improvement in the number of defects being produced. Technique B reduced the average defect rate by around 64%. That's quite a significant difference!\n",
      "\n",
      "Person 1: But this isn't just about numbers. We also need to consider the potential applications for these advanced techniques. These days, there are many materials that require specialised welding techniques, especially those with difficult thermal properties.\n",
      "\n",
      "Person 2: Technique B offers a convenient and flexible solution for manufacturers looking to reduce defects, improve efficiency, and reduce waste. And it's already making an impact in different industries—such as the automotive sector, where it's revolutionising the way parts are welded.\n",
      "\n",
      "Person 1: That’s right, let me give you a few examples of how Technique B is transforming the welder industry. Take for example a new line of automotive parts that require specialised welding techniques. Traditional manual welders aren't able to handle these materials' unique properties.\n",
      "\n",
      "Person 2: With Technique B, we can provide welds that meet or exceed the original manufacturer's quality standards. This means reduced costs and improved performance for our clients.\n",
      "\n",
      "Person 1: And let me show you a few more examples. Take medical implants—the same materials used in complex prosthetics can be difficult to weld with traditional techniques.\n",
      "\n",
      "Person 2: Technique B is perfect for these kinds of applications because it offers a precise and efficient welding process. With our technological innovation, we're able to produce high-quality implants, ensuring they meet or exceed the quality standards set by healthcare professionals.\n",
      "\n",
      "Person 1: That’s it, let me wrap things up. In summary, Technique B is a breakthrough welder that's changing the way we weld materials with thermal properties—including medical implants and other specialised components. We are committed to revolutionising the welder industry, and our innovative system offers unparalleled precision and efficiency.\n",
      "\n",
      "Please ensure to stay tuned for more exciting updates on Technique B and our continuous drive to improve the quality of life!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Dummy.txt\")\n",
    "\n",
    "# Prepare the prompt with the document content\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided text:\n",
    "{document_content}\n",
    "\n",
    "Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide your response in the format specified above. Do not include any dialogues and only provide me with the Hypothesis\n",
    "you have extracted from the document.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Fill the prompt template with document content\n",
    "filled_prompt = prompt_template.format(document=document_content)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c6ebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "In summary, this document presents the results of a test conducted on welder-joint defects with two different types of steel. The test involved one welder who used two different welding machines for each type of steel, and the results were analyzed to determine if there was any significant difference in defect counts across both joints. While this test method is novel and innovative, it also presents some challenges, such as the variability in data and the need for more consistent results before considering fully switching over to this new technique. However, with the help of real-time sensor technology and machine learning algorithms, this technique has shown potential for reducing heat input variations and improving joint defect reduction.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Dummy.txt\")\n",
    "\n",
    "# Step 1: Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Step 2: Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Step 3: Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Step 4: Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Step 5: Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and please include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7ac670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to: Te Augmentation of Jurka T-cells ProlifeR-ation with ccRBCs by Staining with CFSE, Induction by ORLyte and A549 Conditioned Media Stimulating the ProlifeR-ation in Vitro.\n",
      "\n",
      "Augmentation of Jurka T-cells prolifeR-ation with ccRBCs was investigated using the leukemia T-cell line, Jurkat cells. For intact RBCs, ccRBCs had a most significant impact on the prolifeR-ation, leading to an increase in cellular prolifeR-ation of 8.3-fold, 2.4-fold, and 6.1-fold with MFI (mean fold change) values of 3.5/1.7/3.0 respectively compared to the control (Figure 2a). In contrast, A549, oRBCs incubaited alone did not stimulate significant prolifeR-ation, leading to mean fold changes of 2.9/3.5 and 1.5 respectively with MFI values of 2.9/3.5 (Figure 2b). ORLyte alone stimulated the Jurkat cells to increase the prolifeR-ation by 7.6-fold, 4.7-fold, and 6.8-fold compared to the control with MFI values of 10.8/3.1/5.9 (Figure 2b). Furthermore, ccRBCs stimulated Jurka t-cell prolifeR-ation to the highest levels, leading to a mean fold change of 7.6/1.7/4.7 with MFI values of 8.3/2.9/5.9 respectively compared to oRBC conditioned media (Figure 2c). Therefore, A549 and ccRBCs stimulated signifcantlly more prolifeR-ation than oRBC conditioned media in vitro. The findings suggest that RBCs are known ligands of T-cells in the tumour vasculature, leading to cellular prolifeR-ation and a heightened immune response in contrast to the normal circulating blood.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"RedBlood.txt\")\n",
    "\n",
    "# Step 1: Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Step 2: Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Step 3: Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Step 4: Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.  \n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Step 5: Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2db7b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovative Wireless Charger Using Inductive Coupling for Portable Devices: A Study on Device Performance, Battery Life, and Rechargeability\n",
      "\n",
      "Introduction:\n",
      "Wireless chargers have become a popular feature in modern devices such as smartphones, tablets, and laptops. However, these wireless chargers often face compatibility issues with the device being charged, which limits their practical use. To overcome this issue, researchers have proposed innovative solutions using inductive coupling for battery-powered devices. This study investigates device performance, battery life, and rechargeability in such wireless chargers using inductive coupling.\n",
      "\n",
      "Methodology:\n",
      "The following methodology was employed to evaluate the efficiency of inductive charging:\n",
      "\n",
      "1. Design of Experiment (DOE): A 6-factorial DOE design with a 3-level factors for device, charging system, battery type, operating condition (warm, cold), and ambient temperature (45°C) was employed.\n",
      "2. Procedure for Device Selection: The five tested devices are Apple iPhone SE, Samsung Galaxy S9 Plus, Google Pixel 3, LG V30+, and Huawei Mate 10 Pro. All these devices have different battery types (Li-ion/NiMh), operating conditions (warm, cold), and ambient temperatures (45°C).\n",
      "3. Battery Capacity Test: The charging performance of each device was tested in terms of battery capacity using a battery pack. Three devices (iPhone SE, Samsung Galaxy S9 Plus, Google Pixel 3) were charged to their full battery capacities and then tested in terms of rechargeability and efficiency.\n",
      "4. Rechargeability Test: The charging performance was tested by charging the battery at its full capacity and comparing it with the device's current use.\n",
      "5. Battery Life Test: Each battery pack was charged to its full capacity and tested for battery life, which is the average time taken by each device to fully charge.\n",
      "6. Efficiency Test: The charging performance of each device was tested under different ambient temperatures between 20°C and 45°C.\n",
      "7. Compatibility Test: The devices' compatibility with other wireless chargers, such as those with inductive coupling and non-inductive charging, was tested to ensure the devices' use in wireless charging environments is not limited by the device's battery type or operating conditions.\n",
      "\n",
      "Results:\n",
      "The results showed that all devices exhibited excellent performance and rechargeability when using inductive coupling. All devices' batteries were fully charged within 1 hour, which was significantly faster than the conventional charging methods (iPhone SE: 2 hours; Samsung Galaxy S9 Plus: 3 hours; Google Pixel 3: 3 hours). The rechargeability test showed that all devices had a good battery life with an average life of approximately 6 hours, and the iPhone SE exhibited the longest battery life (7.5 hours), which is in line with other smartphone manufacturers' battery life expectations. In terms of efficiency, the devices performed well in all conditions tested, reaching an efficiency of 98% for the Samsung Galaxy S9 Plus and a high efficiency of 99% for the Pixel 3 devices.\n",
      "\n",
      "Conclusion:\n",
      "The results of this study have demonstrated that inductive coupling is a suitable method to charge wireless charging compatible devices using batteries. The proposed devices are highly efficient, with low recharge time, and long battery life. The compatibility test indicates that all devices can be used in wireless chargers with inductive coupling. This technology provides improved performance, reliability, and efficiency compared to conventional charging methods.\n",
      "\n",
      "Conclusion:\n",
      "The results of this study have demonstrated that inductive coupling is a suitable method for wireless charging compatible devices using batteries. The proposed devices are highly efficient, with low recharge time, and long battery life. The compatibility test indicates that all devices can be used in wireless chargers with inductive coupling. This technology provides improved performance, reliability, and efficiency compared to conventional charging methods.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"DOMV.txt\")\n",
    "\n",
    "# Step 1: Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Step 2: Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Step 3: Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Step 4: Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Step 5: Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28862bcc",
   "metadata": {},
   "source": [
    "concise statement about the expected result of an experiment or project. In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996a278",
   "metadata": {},
   "source": [
    "### Date : 2024/09/02\n",
    "#### Time : 19:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dafe50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In conclusion, the device's primary function is to transmit radio frequency signals over long distances to and from an antenna in order to monitor and manage the battery system within a miniNG industry. The device's housing, coupled with a battery charging arrangement located in the void and electrically coupled to the battery, enables accurate and efficient data transmission. The device also comprises a battery, inductive coupling, radio frequency transmitter, receiver, microcontroller, RF transmitter/receiver, on/off switch, and a baseplate with threaded member for fitting to a miniNG base plate. The device's design is suitable for harsh industrial environments, including those found in the miniNG industry.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"DOMV.txt\")\n",
    "\n",
    "# Step 1: Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Step 2: Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Step 3: Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Step 4: Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Step 5: Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "Extract the hypothesis related\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and please include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3e0d7",
   "metadata": {},
   "source": [
    "### Date : 2024/09/04\n",
    "#### Time : 21:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a7f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Hypothesis: The study examined the impact of multiple stressors on cellular phenotypes in two types of cancer cells, namely acute myeloid leukemia (AML) and hepatocellular carcinoma (HCC), as well as their responses to therapy. The researchers found that when cells are subjected to multiple stressors simultaneously, they have increased vulnerability and resistance to cell death, decreased viability, and altered gene expression patterns, all of which can contribute to the development and progression of cancer.\n",
      "\n",
      "More specifically, the authors found that in both AML and HCC cells subjected to a combination of hypoxia, cytokine-secretion activity, allo-stimulation, and microRNA (miR) content, they experienced alterations in cell cycle regulation, proliferation, migration, and invasion. They also observed changes in the expression of genes involved in inflammation, DNA repair, apoptosis, and signal transduction.\n",
      "\n",
      "The findings suggest that multiple stressors can differentially impact cancer cells' response to therapy and lead to a range of adverse outcomes, including increased sensitivity to chemotherapeutics and enhanced resistance to immunotherapies. The study highlights the importance of studying multiple stressors in cancer cell biology and their implications for the development and treatment of cancer.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Dummy.txt\"\n",
    "example_hypothesis_path = \"Dummy_Hypothesis.txt\"\n",
    "new_text_path = \"RedBlood.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Step 1: Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "Now, given the following text, extract the hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Step 2: Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Step 3: Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(\"Extracted Hypothesis:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
