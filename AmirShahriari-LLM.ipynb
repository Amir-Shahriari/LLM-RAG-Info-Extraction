{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015976fb-39fe-4625-9290-d6e523de5eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (1.5 GiB) than is available (1.3 GiB)\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(document\u001b[38;5;241m=\u001b[39mdocument_content)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Invoke the model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:271\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m         )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines(decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (1.5 GiB) than is available (1.3 GiB)\"}"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://jupyter.weburban.com:12434\",\n",
    "    model=\"qwen:0.5b\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Data/Dummy.txt\")\n",
    "\n",
    "# Prepare the prompt with the document content\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided text:\n",
    "{document_content}\n",
    "\n",
    "Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide your response in the format specified above. Do not include any dialogues and only provide me with the Hypothesis\n",
    "you have extracted from the document.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Fill the prompt template with document content\n",
    "filled_prompt = prompt_template.format(document=document_content)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c6ebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "In summary, this document presents the results of a test conducted on welder-joint defects with two different types of steel. The test involved one welder who used two different welding machines for each type of steel, and the results were analyzed to determine if there was any significant difference in defect counts across both joints. While this test method is novel and innovative, it also presents some challenges, such as the variability in data and the need for more consistent results before considering fully switching over to this new technique. However, with the help of real-time sensor technology and machine learning algorithms, this technique has shown potential for reducing heat input variations and improving joint defect reduction.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Dummy.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and please include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7ac670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to: Te Augmentation of Jurka T-cells ProlifeR-ation with ccRBCs by Staining with CFSE, Induction by ORLyte and A549 Conditioned Media Stimulating the ProlifeR-ation in Vitro.\n",
      "\n",
      "Augmentation of Jurka T-cells prolifeR-ation with ccRBCs was investigated using the leukemia T-cell line, Jurkat cells. For intact RBCs, ccRBCs had a most significant impact on the prolifeR-ation, leading to an increase in cellular prolifeR-ation of 8.3-fold, 2.4-fold, and 6.1-fold with MFI (mean fold change) values of 3.5/1.7/3.0 respectively compared to the control (Figure 2a). In contrast, A549, oRBCs incubaited alone did not stimulate significant prolifeR-ation, leading to mean fold changes of 2.9/3.5 and 1.5 respectively with MFI values of 2.9/3.5 (Figure 2b). ORLyte alone stimulated the Jurkat cells to increase the prolifeR-ation by 7.6-fold, 4.7-fold, and 6.8-fold compared to the control with MFI values of 10.8/3.1/5.9 (Figure 2b). Furthermore, ccRBCs stimulated Jurka t-cell prolifeR-ation to the highest levels, leading to a mean fold change of 7.6/1.7/4.7 with MFI values of 8.3/2.9/5.9 respectively compared to oRBC conditioned media (Figure 2c). Therefore, A549 and ccRBCs stimulated signifcantlly more prolifeR-ation than oRBC conditioned media in vitro. The findings suggest that RBCs are known ligands of T-cells in the tumour vasculature, leading to cellular prolifeR-ation and a heightened immune response in contrast to the normal circulating blood.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"RedBlood.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.  \n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2db7b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovative Wireless Charger Using Inductive Coupling for Portable Devices: A Study on Device Performance, Battery Life, and Rechargeability\n",
      "\n",
      "Introduction:\n",
      "Wireless chargers have become a popular feature in modern devices such as smartphones, tablets, and laptops. However, these wireless chargers often face compatibility issues with the device being charged, which limits their practical use. To overcome this issue, researchers have proposed innovative solutions using inductive coupling for battery-powered devices. This study investigates device performance, battery life, and rechargeability in such wireless chargers using inductive coupling.\n",
      "\n",
      "Methodology:\n",
      "The following methodology was employed to evaluate the efficiency of inductive charging:\n",
      "\n",
      "1. Design of Experiment (DOE): A 6-factorial DOE design with a 3-level factors for device, charging system, battery type, operating condition (warm, cold), and ambient temperature (45°C) was employed.\n",
      "2. Procedure for Device Selection: The five tested devices are Apple iPhone SE, Samsung Galaxy S9 Plus, Google Pixel 3, LG V30+, and Huawei Mate 10 Pro. All these devices have different battery types (Li-ion/NiMh), operating conditions (warm, cold), and ambient temperatures (45°C).\n",
      "3. Battery Capacity Test: The charging performance of each device was tested in terms of battery capacity using a battery pack. Three devices (iPhone SE, Samsung Galaxy S9 Plus, Google Pixel 3) were charged to their full battery capacities and then tested in terms of rechargeability and efficiency.\n",
      "4. Rechargeability Test: The charging performance was tested by charging the battery at its full capacity and comparing it with the device's current use.\n",
      "5. Battery Life Test: Each battery pack was charged to its full capacity and tested for battery life, which is the average time taken by each device to fully charge.\n",
      "6. Efficiency Test: The charging performance of each device was tested under different ambient temperatures between 20°C and 45°C.\n",
      "7. Compatibility Test: The devices' compatibility with other wireless chargers, such as those with inductive coupling and non-inductive charging, was tested to ensure the devices' use in wireless charging environments is not limited by the device's battery type or operating conditions.\n",
      "\n",
      "Results:\n",
      "The results showed that all devices exhibited excellent performance and rechargeability when using inductive coupling. All devices' batteries were fully charged within 1 hour, which was significantly faster than the conventional charging methods (iPhone SE: 2 hours; Samsung Galaxy S9 Plus: 3 hours; Google Pixel 3: 3 hours). The rechargeability test showed that all devices had a good battery life with an average life of approximately 6 hours, and the iPhone SE exhibited the longest battery life (7.5 hours), which is in line with other smartphone manufacturers' battery life expectations. In terms of efficiency, the devices performed well in all conditions tested, reaching an efficiency of 98% for the Samsung Galaxy S9 Plus and a high efficiency of 99% for the Pixel 3 devices.\n",
      "\n",
      "Conclusion:\n",
      "The results of this study have demonstrated that inductive coupling is a suitable method to charge wireless charging compatible devices using batteries. The proposed devices are highly efficient, with low recharge time, and long battery life. The compatibility test indicates that all devices can be used in wireless chargers with inductive coupling. This technology provides improved performance, reliability, and efficiency compared to conventional charging methods.\n",
      "\n",
      "Conclusion:\n",
      "The results of this study have demonstrated that inductive coupling is a suitable method for wireless charging compatible devices using batteries. The proposed devices are highly efficient, with low recharge time, and long battery life. The compatibility test indicates that all devices can be used in wireless chargers with inductive coupling. This technology provides improved performance, reliability, and efficiency compared to conventional charging methods.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"DOMV.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "The hypothesis should be a statement or series of statements that outline (a) the result you aim to achieve, and (b) how and why you believe you can achieve it, informed by your background research.\n",
    "\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28862bcc",
   "metadata": {},
   "source": [
    "concise statement about the expected result of an experiment or project. In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996a278",
   "metadata": {},
   "source": [
    "### Date : 2024/09/02\n",
    "#### Time : 19:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dafe50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In conclusion, the device's primary function is to transmit radio frequency signals over long distances to and from an antenna in order to monitor and manage the battery system within a miniNG industry. The device's housing, coupled with a battery charging arrangement located in the void and electrically coupled to the battery, enables accurate and efficient data transmission. The device also comprises a battery, inductive coupling, radio frequency transmitter, receiver, microcontroller, RF transmitter/receiver, on/off switch, and a baseplate with threaded member for fitting to a miniNG base plate. The device's design is suitable for harsh industrial environments, including those found in the miniNG industry.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document  \n",
    "from langchain.retrievers import TFIDFRetriever \n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"DOMV.txt\")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = document_content.split(\"\\n\\n\")  # Example: splitting by paragraphs\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)  # Correct way to initialize with Document objects\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"Please extract and clearly label the **Hypothesis** from the document content.\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)  # Retrieve relevant chunks\n",
    "\n",
    "# Combine relevant chunks\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided relevant text:\n",
    "{relevant_text}\n",
    "\n",
    "Extract the hypothesis related\n",
    "**Format for Response**:\n",
    "Your response should strictly follow the format provided above and include only the hypothesis.\n",
    "\n",
    "Hypothesis:\n",
    "1. [Your hypothesis statement(s) here]\n",
    "\n",
    "Please provide **Hypothesis** of this provided document in only a single paragraph in the format specified above and please include the title of the paragraph as **Hypothesis**.\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with retrieved text\n",
    "filled_prompt = prompt_template.format(document=relevant_text)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3e0d7",
   "metadata": {},
   "source": [
    "### Date : 2024/09/04\n",
    "#### Time : 21:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a7f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Hypothesis: The study examined the impact of multiple stressors on cellular phenotypes in two types of cancer cells, namely acute myeloid leukemia (AML) and hepatocellular carcinoma (HCC), as well as their responses to therapy. The researchers found that when cells are subjected to multiple stressors simultaneously, they have increased vulnerability and resistance to cell death, decreased viability, and altered gene expression patterns, all of which can contribute to the development and progression of cancer.\n",
      "\n",
      "More specifically, the authors found that in both AML and HCC cells subjected to a combination of hypoxia, cytokine-secretion activity, allo-stimulation, and microRNA (miR) content, they experienced alterations in cell cycle regulation, proliferation, migration, and invasion. They also observed changes in the expression of genes involved in inflammation, DNA repair, apoptosis, and signal transduction.\n",
      "\n",
      "The findings suggest that multiple stressors can differentially impact cancer cells' response to therapy and lead to a range of adverse outcomes, including increased sensitivity to chemotherapeutics and enhanced resistance to immunotherapies. The study highlights the importance of studying multiple stressors in cancer cell biology and their implications for the development and treatment of cancer.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Dummy.txt\"\n",
    "example_hypothesis_path = \"Dummy_Hypothesis.txt\"\n",
    "new_text_path = \"RedBlood.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "Now, given the following text, extract the hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3490bb2",
   "metadata": {},
   "source": [
    "### Date : 2024/09/07\n",
    "#### Time : 19:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55524ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA201A010>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001DDA201A010>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA201A010>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mnew_text)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA201A010>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "Now, given the following text, extract the hypothesis, remember to label the extracted chunk as Hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c50b0fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DD929B1BD0>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001DD929B1BD0>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DD929B1BD0>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mnew_text)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DD929B1BD0>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "**Definition of Hypothesis**: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "Now, given the following text, extract the hypothesis, remember to label the extracted chunk as Hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b6d37",
   "metadata": {},
   "source": [
    "### Date : 2024/09/10\n",
    "#### Time : 20:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c9d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the device as claimed in claim 7 of the patent application meets all requirements for a viability sensor. The housing has an inner surface defined by a void that receives a coupling arrangement and extends outwardly from a main surface of the cover, which has a threaded section for receiving a correspondingly threaded member fixed to the baseplate. The coupling arrangement is located in the void, allowing it to be removably fitted to the housing. The viability sensor is coupled to the coupling arrangement and electrically coupled to the electronic circuit located in the void. The electronic circuit is arranged to receive a signal from the sensor and process the signal to create viability data. The device also includes a battery, an on/off switch, and a plurality of protusions for fixing the device to a baseplate.\n",
      "\n",
      "The device as claimed in any one of the preceding claims meets all requirements for a viability sensor. Its housing has a void that receives a coupling arrangement and extends outwardly from a main surface of the cover, allowing it to be removably fitted to the housing. The electronic circuit is located in the void, allowing it to receive a signal from the sensor and process the signal to create viability data. The device also includes a battery, an on/off switch, and a plurality of protusions for fixing the device to a baseplate.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "Definition of Hypothesis: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "Now, given the following text, extract the hypothesis, remember to label the extracted chunk as Hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49aee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: So, how did you ensure that everything else was kept the same while welding steel plates?\n",
      "\n",
      "Person 2: We used 10mm thick steel plate and made sure it was high-grade.\n",
      "\n",
      "Person 3: What is the Welder's Inspector, and what kind of inspection tools do they use to ensure consistent results?\n",
      "\n",
      "Person 1: The Welder's Inspector checks that the welder's tools are calibrated correctly and that there is no deviation from the established pattern. They also monitor the welder's performance during the process, ensuring consistency in the number of defects per joint.\n",
      "\n",
      "Person 2: We used ultrasonic testing equipment to ensure that the steel plates were not contaminated with other metals or impurities, and we also checked for small inconsistencies. But even then, there were still a few outliers, and it's hard to say which ones are due to defective material and which are due to human error.\n",
      "\n",
      "Person 3: The Welder's Inspector uses statistical significance to determine if the new technique's results are consistent with what we saw before. While we did a t-test, there is still a margin of error, and we need more data to make a firm conclusion.\n",
      "\n",
      "Person 1: So, it looks like we'll have to wait for more data to determine if this new welding technique is truly worth implementing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Load texts from files\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "new_text_path = \"Data/Dummy.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Prepare the prompt with example and request for new hypothesis extraction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Definition of Hypothesis: \n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project.\n",
    "In many ways, a research hypothesis represents the starting point for a scientific endeavor, as it establishes\n",
    "a tentative assumption that is eventually substantiated or falsified, ultimately improving our certainty about the subject investigated.\n",
    "\n",
    "Here is an example text with its hypothesis:\n",
    "\n",
    "Text: {example_text}\n",
    "Hypothesis: {example_hypothesis}\n",
    "\n",
    "\n",
    "\n",
    "Now, given the following text, extract the hypothesis, remember to label the extracted chunk as Hypothesis:\n",
    "\n",
    "Text: {{text}}\n",
    "Hypothesis: \n",
    "\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text loaded from the file\n",
    "filled_prompt = prompt_template.format(text=new_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64862ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embodiement of a device for collecting vibraction data comprises a housing with an inner surface defining a void, a coupling arrangement for coupling the housing to a baseplate, a vibraction sensor located in the void, and an electronic circuit arranged to receive signals from the sensor and process them to create vibraction data. The device is formed from nylon polyamide and IP68 standards, allowing it to be suitable for use in harsh environments.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load and prepare document\n",
    "document_content = load_text_file(\"Data/device.txt\")\n",
    "\n",
    "# Split into sentences for more precise chunking\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "chunks = sent_tokenize(document_content)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Define query with a refined approach\n",
    "query = \"\"\"\n",
    "Please extract and clearly label the **Hypothesis** from the document content. \n",
    "A hypothesis is a statement that proposes a potential explanation for a phenomenon or a specific outcome of an experiment. \n",
    "It is a concise and testable prediction.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Hypothesis:\n",
    "1. [Extracted hypothesis here]\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve relevant chunks and prepare prompt\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Given the following relevant text:\n",
    "    {relevant_text}\n",
    "\n",
    "    Extract the hypothesis as defined below:\n",
    "    A research hypothesis is a statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "    **Hypothesis:**\n",
    "    1. [Extracted hypothesis here]\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6ba96",
   "metadata": {},
   "source": [
    "### Date : 2024/09/12\n",
    "#### Time : 11:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8b0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment supports the hypothesis that regular aerobic exercise (e.g., running or cycling) enhances cognitive function, particularly in memory and executive control tasks, in adults aged 25-50. The improvements observed are evident in memory-related tasks and tasks requiring executive control, such as planning and problem-solving, which can be attributed to the positive effects of physical activity on brain health. These findings provide evidence that regular aerobic exercise is an effective intervention for enhancing cognitive function in adults, which could have significant implications for daily life and professional domains.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Initialize the model (TinyLlama in this case)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "query = \"\"\"\n",
    "Please extract and clearly label the **Hypothesis** from the document content. \n",
    "A hypothesis is a statement that proposes a potential explanation for a phenomenon or a specific outcome of an experiment. \n",
    "It is a concise and testable prediction.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Hypothesis:\n",
    "1. [Extracted hypothesis here]\n",
    "\"\"\"\n",
    "\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "    Now, given the following relevant text, extract the hypothesis:\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Hypothesis:**\n",
    "    1. [Extracted hypothesis here]\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the new text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f2a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Ollama call failed with status code 403. Details: <!DOCTYPE HTML>\n<html>\n\n<head>\n\n    <meta charset=\"utf-8\">\n\n    <title>Jupyter Server</title>\n    <link id=\"favicon\" rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/static/favicon.ico?v=50afa725b5de8b00030139d09b38620224d4e7dba47c07ef0e86d4643f30c9bfe6bb7e1a4a1c561aa32834480909a4b6fe7cd1e17f7159330b6b5914bf45a880\">\n    \n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap.min.css?v=0e8a7fbd6de23ad6b27ab95802a0a0915af6693af612bc304d83af445529ce5d95842309ca3405d10f538d45c8a3a261b8cff78b4bd512dd9effb4109a71d0ab\" />\n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap-theme.min.css?v=8b2f045cb5b4d5ad346f6e816aa2566829a4f5f2783ec31d80d46a57de8ac0c3d21fe6e53bcd8e1f38ac17fcd06d12088bc9b43e23b5d1da52d10c6b717b22b3\" />\n    <link rel=\"stylesheet\" href=\"/static/style/index.css?v=30372e3246a801d662cf9e3f9dd656fa192eebde9054a2282449fe43919de9f0ee9b745d7eb49d3b0a5e56357912cc7d776390eddcab9dac85b77bdb17b4bdae\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\n    \n\n    \n<style type=\"text/css\">\n    /* disable initial hide */\n    div#header,\n    div#site {\n        display: block;\n    }\n</style>\n\n\n    \n    \n\n</head>\n\n<body class=\"\"    dir=\"ltr\">\n\n  <noscript>\n    <div id='noscript'>\n      Jupyter Server requires JavaScript.<br>\n      Please enable it to proceed. \n    </div>\n  </noscript>\n\n  <div id=\"header\" role=\"navigation\" aria-label=\"Top Menu\">\n    <div id=\"header-container\" class=\"container\">\n      <div id=\"jupyter_server\" class=\"nav navbar-brand\"><a href=\"/lab\" title='dashboard'>\n          <img src='/static/logo/logo.png?v=a2a176ee3cee251ffddf5fa21fe8e43727a9e5f87a06f9c91ad7b776d9e9d3d5e0159c16cc188a3965e00375fb4bc336c16067c688f5040c0c2d4bfdb852a9e4' alt='Jupyter Server' />\n        </a></div>\n\n      \n      \n\n      \n      \n\n    </div>\n    <div class=\"header-bar\"></div>\n\n    \n    \n  </div>\n\n  <div id=\"site\">\n    \n\n<div class=\"error\">\n    \n    <h1>403 : Forbidden</h1>\n    \n    \n    \n    <p>The error was:</p>\n    <div class=\"traceback-wrapper\">\n        <pre class=\"traceback\">&#39;_xsrf&#39; argument missing from POST</pre>\n    </div>\n    \n    \n</div>\n\n\n  </div>\n\n  \n  \n\n  \n\n\n  <script type='text/javascript'>\n    function _remove_token_from_url() {\n      if (window.location.search.length <= 1) {\n        return;\n      }\n      var search_parameters = window.location.search.slice(1).split('&');\n      for (var i = 0; i < search_parameters.length; i++) {\n        if (search_parameters[i].split('=')[0] === 'token') {\n          // remote token from search parameters\n          search_parameters.splice(i, 1);\n          var new_search = '';\n          if (search_parameters.length) {\n            new_search = '?' + search_parameters.join('&');\n          }\n          var new_url = window.location.origin +\n            window.location.pathname +\n            new_search +\n            window.location.hash;\n          window.history.replaceState({}, \"\", new_url);\n          return;\n        }\n      }\n    }\n    _remove_token_from_url();\n  </script>\n</body>\n\n</html>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mrelevant_text)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:271\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m         )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines(decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Ollama call failed with status code 403. Details: <!DOCTYPE HTML>\n<html>\n\n<head>\n\n    <meta charset=\"utf-8\">\n\n    <title>Jupyter Server</title>\n    <link id=\"favicon\" rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/static/favicon.ico?v=50afa725b5de8b00030139d09b38620224d4e7dba47c07ef0e86d4643f30c9bfe6bb7e1a4a1c561aa32834480909a4b6fe7cd1e17f7159330b6b5914bf45a880\">\n    \n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap.min.css?v=0e8a7fbd6de23ad6b27ab95802a0a0915af6693af612bc304d83af445529ce5d95842309ca3405d10f538d45c8a3a261b8cff78b4bd512dd9effb4109a71d0ab\" />\n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap-theme.min.css?v=8b2f045cb5b4d5ad346f6e816aa2566829a4f5f2783ec31d80d46a57de8ac0c3d21fe6e53bcd8e1f38ac17fcd06d12088bc9b43e23b5d1da52d10c6b717b22b3\" />\n    <link rel=\"stylesheet\" href=\"/static/style/index.css?v=30372e3246a801d662cf9e3f9dd656fa192eebde9054a2282449fe43919de9f0ee9b745d7eb49d3b0a5e56357912cc7d776390eddcab9dac85b77bdb17b4bdae\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\n    \n\n    \n<style type=\"text/css\">\n    /* disable initial hide */\n    div#header,\n    div#site {\n        display: block;\n    }\n</style>\n\n\n    \n    \n\n</head>\n\n<body class=\"\"    dir=\"ltr\">\n\n  <noscript>\n    <div id='noscript'>\n      Jupyter Server requires JavaScript.<br>\n      Please enable it to proceed. \n    </div>\n  </noscript>\n\n  <div id=\"header\" role=\"navigation\" aria-label=\"Top Menu\">\n    <div id=\"header-container\" class=\"container\">\n      <div id=\"jupyter_server\" class=\"nav navbar-brand\"><a href=\"/lab\" title='dashboard'>\n          <img src='/static/logo/logo.png?v=a2a176ee3cee251ffddf5fa21fe8e43727a9e5f87a06f9c91ad7b776d9e9d3d5e0159c16cc188a3965e00375fb4bc336c16067c688f5040c0c2d4bfdb852a9e4' alt='Jupyter Server' />\n        </a></div>\n\n      \n      \n\n      \n      \n\n    </div>\n    <div class=\"header-bar\"></div>\n\n    \n    \n  </div>\n\n  <div id=\"site\">\n    \n\n<div class=\"error\">\n    \n    <h1>403 : Forbidden</h1>\n    \n    \n    \n    <p>The error was:</p>\n    <div class=\"traceback-wrapper\">\n        <pre class=\"traceback\">&#39;_xsrf&#39; argument missing from POST</pre>\n    </div>\n    \n    \n</div>\n\n\n  </div>\n\n  \n  \n\n  \n\n\n  <script type='text/javascript'>\n    function _remove_token_from_url() {\n      if (window.location.search.length <= 1) {\n        return;\n      }\n      var search_parameters = window.location.search.slice(1).split('&');\n      for (var i = 0; i < search_parameters.length; i++) {\n        if (search_parameters[i].split('=')[0] === 'token') {\n          // remote token from search parameters\n          search_parameters.splice(i, 1);\n          var new_search = '';\n          if (search_parameters.length) {\n            new_search = '?' + search_parameters.join('&');\n          }\n          var new_url = window.location.origin +\n            window.location.pathname +\n            new_search +\n            window.location.hash;\n          window.history.replaceState({}, \"\", new_url);\n          return;\n        }\n      }\n    }\n    _remove_token_from_url();\n  </script>\n</body>\n\n</html>"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://jupyter.weburban.com:8870\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract text related to hypotheses from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
    "\n",
    "    **Hypothesis:**\n",
    "    1. [Extracted hypothesis here]\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e77578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Secretion of cytokines from PBMCs following RBC treatment.\n",
      "Haemolysis during sample preparation alters microRNA content of plasma.\n",
      "Tese results support the hypothesis that RBCs play a key role in modulating the immune system.\n",
      "Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "Filled Prompt:\n",
      "\n",
      "    Here is an example of a text with its corresponding hypothesis:\n",
      "\n",
      "    **Example Text:**\n",
      "    Participant Selection:\n",
      "\n",
      "Recruit 100 adults aged 25-50.\n",
      "Split the participants into two groups:\n",
      "Experimental group: Engages in 30 minutes of aerobic exercise (e.g., running or cycling) 5 times a week for 12 weeks.\n",
      "Control group: No regular exercise during the same period.\n",
      "Cognitive Function Tests:\n",
      "\n",
      "Measure participants' baseline cognitive abilities using a standardized battery of cognitive tests (focusing on memory recall, problem-solving, and executive control).\n",
      "Administer the same tests at intervals: before starting the regimen, at 6 weeks, and at the end of 12 weeks.\n",
      "Physiological Measurements:\n",
      "\n",
      "Track physical health improvements (heart rate, VO2 max) in the exercise group to ensure that aerobic capacity is improving alongside cognitive measures.\n",
      "\n",
      "\n",
      "Regular aerobic exercise enhances cognitive function, especially in tasks involving memory and executive control, in adults between the ages of 25 and 50.\n",
      "\n",
      "Cognitive Test Scores:\n",
      "\n",
      "Over the 12-week period, participants in the experimental group show an improvement in memory recall and executive function tasks compared to their baseline scores.\n",
      "The control group shows little to no improvement or a slight decline in cognitive function over time.\n",
      "Physiological Improvement:\n",
      "\n",
      "The experimental group shows increased aerobic capacity (measured through VO2 max) and lower resting heart rates, indicating enhanced physical fitness.\n",
      "Qualitative Feedback:\n",
      "\n",
      "Some participants in the experimental group report feeling more alert and focused in daily tasks, while control group participants report no significant changes.\n",
      "\n",
      "The experimental group exhibits a statistically significant improvement in cognitive function compared to the control group. The improvements are particularly noticeable in memory-related tasks and tasks requiring executive control (like planning and problem-solving).\n",
      "The correlation between improved physical fitness and cognitive enhancement supports the idea that aerobic exercise boosts brain function.\n",
      "\n",
      "The experiment supports the hypothesis that regular aerobic exercise improves cognitive function in adults aged 25-50. The increase in cognitive abilities, particularly in memory and executive control, can be attributed to the positive effects of physical activity on brain health. Hence, aerobic exercise should be considered a beneficial intervention for enhancing cognitive performance in adults.\n",
      "    \n",
      "    **Example Hypothesis:**\n",
      "    Engaging in regular aerobic exercise improves cognitive function, particularly in tasks related to memory and executive control, in adults aged 25-50.\n",
      "\n",
      "\n",
      "    \n",
      "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
      "    \n",
      "    **Definition of Hypothesis:**\n",
      "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
      "\n",
      "    **Relevant Text:**\n",
      "    Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
      "    Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "    \n",
      "    **Hypothesis:**\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Ollama call failed with status code 403. Details: <!DOCTYPE HTML>\n<html>\n\n<head>\n\n    <meta charset=\"utf-8\">\n\n    <title>Jupyter Server</title>\n    <link id=\"favicon\" rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/static/favicon.ico?v=50afa725b5de8b00030139d09b38620224d4e7dba47c07ef0e86d4643f30c9bfe6bb7e1a4a1c561aa32834480909a4b6fe7cd1e17f7159330b6b5914bf45a880\">\n    \n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap.min.css?v=0e8a7fbd6de23ad6b27ab95802a0a0915af6693af612bc304d83af445529ce5d95842309ca3405d10f538d45c8a3a261b8cff78b4bd512dd9effb4109a71d0ab\" />\n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap-theme.min.css?v=8b2f045cb5b4d5ad346f6e816aa2566829a4f5f2783ec31d80d46a57de8ac0c3d21fe6e53bcd8e1f38ac17fcd06d12088bc9b43e23b5d1da52d10c6b717b22b3\" />\n    <link rel=\"stylesheet\" href=\"/static/style/index.css?v=30372e3246a801d662cf9e3f9dd656fa192eebde9054a2282449fe43919de9f0ee9b745d7eb49d3b0a5e56357912cc7d776390eddcab9dac85b77bdb17b4bdae\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\n    \n\n    \n<style type=\"text/css\">\n    /* disable initial hide */\n    div#header,\n    div#site {\n        display: block;\n    }\n</style>\n\n\n    \n    \n\n</head>\n\n<body class=\"\"    dir=\"ltr\">\n\n  <noscript>\n    <div id='noscript'>\n      Jupyter Server requires JavaScript.<br>\n      Please enable it to proceed. \n    </div>\n  </noscript>\n\n  <div id=\"header\" role=\"navigation\" aria-label=\"Top Menu\">\n    <div id=\"header-container\" class=\"container\">\n      <div id=\"jupyter_server\" class=\"nav navbar-brand\"><a href=\"/lab\" title='dashboard'>\n          <img src='/static/logo/logo.png?v=a2a176ee3cee251ffddf5fa21fe8e43727a9e5f87a06f9c91ad7b776d9e9d3d5e0159c16cc188a3965e00375fb4bc336c16067c688f5040c0c2d4bfdb852a9e4' alt='Jupyter Server' />\n        </a></div>\n\n      \n      \n\n      \n      \n\n    </div>\n    <div class=\"header-bar\"></div>\n\n    \n    \n  </div>\n\n  <div id=\"site\">\n    \n\n<div class=\"error\">\n    \n    <h1>403 : Forbidden</h1>\n    \n    \n    \n    <p>The error was:</p>\n    <div class=\"traceback-wrapper\">\n        <pre class=\"traceback\">&#39;_xsrf&#39; argument missing from POST</pre>\n    </div>\n    \n    \n</div>\n\n\n  </div>\n\n  \n  \n\n  \n\n\n  <script type='text/javascript'>\n    function _remove_token_from_url() {\n      if (window.location.search.length <= 1) {\n        return;\n      }\n      var search_parameters = window.location.search.slice(1).split('&');\n      for (var i = 0; i < search_parameters.length; i++) {\n        if (search_parameters[i].split('=')[0] === 'token') {\n          // remote token from search parameters\n          search_parameters.splice(i, 1);\n          var new_search = '';\n          if (search_parameters.length) {\n            new_search = '?' + search_parameters.join('&');\n          }\n          var new_url = window.location.origin +\n            window.location.pathname +\n            new_search +\n            window.location.hash;\n          window.history.replaceState({}, \"\", new_url);\n          return;\n        }\n      }\n    }\n    _remove_token_from_url();\n  </script>\n</body>\n\n</html>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(filled_prompt[:])  \n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:271\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m         )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines(decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Ollama call failed with status code 403. Details: <!DOCTYPE HTML>\n<html>\n\n<head>\n\n    <meta charset=\"utf-8\">\n\n    <title>Jupyter Server</title>\n    <link id=\"favicon\" rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/static/favicon.ico?v=50afa725b5de8b00030139d09b38620224d4e7dba47c07ef0e86d4643f30c9bfe6bb7e1a4a1c561aa32834480909a4b6fe7cd1e17f7159330b6b5914bf45a880\">\n    \n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap.min.css?v=0e8a7fbd6de23ad6b27ab95802a0a0915af6693af612bc304d83af445529ce5d95842309ca3405d10f538d45c8a3a261b8cff78b4bd512dd9effb4109a71d0ab\" />\n    <link rel=\"stylesheet\" href=\"/static/style/bootstrap-theme.min.css?v=8b2f045cb5b4d5ad346f6e816aa2566829a4f5f2783ec31d80d46a57de8ac0c3d21fe6e53bcd8e1f38ac17fcd06d12088bc9b43e23b5d1da52d10c6b717b22b3\" />\n    <link rel=\"stylesheet\" href=\"/static/style/index.css?v=30372e3246a801d662cf9e3f9dd656fa192eebde9054a2282449fe43919de9f0ee9b745d7eb49d3b0a5e56357912cc7d776390eddcab9dac85b77bdb17b4bdae\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\n    \n\n    \n<style type=\"text/css\">\n    /* disable initial hide */\n    div#header,\n    div#site {\n        display: block;\n    }\n</style>\n\n\n    \n    \n\n</head>\n\n<body class=\"\"    dir=\"ltr\">\n\n  <noscript>\n    <div id='noscript'>\n      Jupyter Server requires JavaScript.<br>\n      Please enable it to proceed. \n    </div>\n  </noscript>\n\n  <div id=\"header\" role=\"navigation\" aria-label=\"Top Menu\">\n    <div id=\"header-container\" class=\"container\">\n      <div id=\"jupyter_server\" class=\"nav navbar-brand\"><a href=\"/lab\" title='dashboard'>\n          <img src='/static/logo/logo.png?v=a2a176ee3cee251ffddf5fa21fe8e43727a9e5f87a06f9c91ad7b776d9e9d3d5e0159c16cc188a3965e00375fb4bc336c16067c688f5040c0c2d4bfdb852a9e4' alt='Jupyter Server' />\n        </a></div>\n\n      \n      \n\n      \n      \n\n    </div>\n    <div class=\"header-bar\"></div>\n\n    \n    \n  </div>\n\n  <div id=\"site\">\n    \n\n<div class=\"error\">\n    \n    <h1>403 : Forbidden</h1>\n    \n    \n    \n    <p>The error was:</p>\n    <div class=\"traceback-wrapper\">\n        <pre class=\"traceback\">&#39;_xsrf&#39; argument missing from POST</pre>\n    </div>\n    \n    \n</div>\n\n\n  </div>\n\n  \n  \n\n  \n\n\n  <script type='text/javascript'>\n    function _remove_token_from_url() {\n      if (window.location.search.length <= 1) {\n        return;\n      }\n      var search_parameters = window.location.search.slice(1).split('&');\n      for (var i = 0; i < search_parameters.length; i++) {\n        if (search_parameters[i].split('=')[0] === 'token') {\n          // remote token from search parameters\n          search_parameters.splice(i, 1);\n          var new_search = '';\n          if (search_parameters.length) {\n            new_search = '?' + search_parameters.join('&');\n          }\n          var new_url = window.location.origin +\n            window.location.pathname +\n            new_search +\n            window.location.hash;\n          window.history.replaceState({}, \"\", new_url);\n          return;\n        }\n      }\n    }\n    _remove_token_from_url();\n  </script>\n</body>\n\n</html>"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://jupyter.weburban.com:8870\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "print(\"\\nFilled Prompt:\")\n",
    "print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5bb81",
   "metadata": {},
   "source": [
    "### Date : 2024/09/13\n",
    "#### Time : 12:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c692272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group.\n",
      "The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions.\n",
      "Both groups will undergo the same curriculum but with different learning supports.\n",
      "Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Both groups will undergo the same curriculum but with different learning supports. Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "\n",
      "Filled Prompt:\n",
      "\n",
      "    Here is an example of a text with its corresponding hypothesis:\n",
      "\n",
      "    **Example Text:**\n",
      "    Participant Selection:\n",
      "\n",
      "Recruit 100 adults aged 25-50.\n",
      "Split the participants into two groups:\n",
      "Experimental group: Engages in 30 minutes of aerobic exercise (e.g., running or cycling) 5 times a week for 12 weeks.\n",
      "Control group: No regular exercise during the same period.\n",
      "Cognitive Function Tests:\n",
      "\n",
      "Measure participants' baseline cognitive abilities using a standardized battery of cognitive tests (focusing on memory recall, problem-solving, and executive control).\n",
      "Administer the same tests at intervals: before starting the regimen, at 6 weeks, and at the end of 12 weeks.\n",
      "Physiological Measurements:\n",
      "\n",
      "Track physical health improvements (heart rate, VO2 max) in the exercise group to ensure that aerobic capacity is improving alongside cognitive measures.\n",
      "\n",
      "\n",
      "Regular aerobic exercise enhances cognitive function, especially in tasks involving memory and executive control, in adults between the ages of 25 and 50.\n",
      "\n",
      "Cognitive Test Scores:\n",
      "\n",
      "Over the 12-week period, participants in the experimental group show an improvement in memory recall and executive function tasks compared to their baseline scores.\n",
      "The control group shows little to no improvement or a slight decline in cognitive function over time.\n",
      "Physiological Improvement:\n",
      "\n",
      "The experimental group shows increased aerobic capacity (measured through VO2 max) and lower resting heart rates, indicating enhanced physical fitness.\n",
      "Qualitative Feedback:\n",
      "\n",
      "Some participants in the experimental group report feeling more alert and focused in daily tasks, while control group participants report no significant changes.\n",
      "\n",
      "The experimental group exhibits a statistically significant improvement in cognitive function compared to the control group. The improvements are particularly noticeable in memory-related tasks and tasks requiring executive control (like planning and problem-solving).\n",
      "The correlation between improved physical fitness and cognitive enhancement supports the idea that aerobic exercise boosts brain function.\n",
      "\n",
      "The experiment supports the hypothesis that regular aerobic exercise improves cognitive function in adults aged 25-50. The increase in cognitive abilities, particularly in memory and executive control, can be attributed to the positive effects of physical activity on brain health. Hence, aerobic exercise should be considered a beneficial intervention for enhancing cognitive performance in adults.\n",
      "    \n",
      "    **Example Hypothesis:**\n",
      "    Engaging in regular aerobic exercise improves cognitive function, particularly in tasks related to memory and executive control, in adults aged 25-50.\n",
      "\n",
      "\n",
      "    \n",
      "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
      "    \n",
      "    **Definition of Hypothesis:**\n",
      "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
      "\n",
      "    **Relevant Text:**\n",
      "    Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Both groups will undergo the same curriculum but with different learning supports. Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "\n",
      "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
      "    Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Both groups will undergo the same curriculum but with different learning supports. Engagement surveys show higher levels of interest and motivation among students in the AI group.\n",
      "    \n",
      "    **Hypothesis:**\n",
      "    \n",
      "The provided example hypothesis is derived from the relevant text, specifically, over the course of a semester, the Artificial Intelligence (AI) group demonstrates a marked improvement in test scores compared to the control group. The AI group receives personalized learning recommendation based on their performance, learning style, and pace, while the control group follows standard instruction without personalized suggestions. This suggests that by utilizing technology to provide more targeted and effective learning supports, students in the AI group may demonstrate higher levels of interest and motivation for the same curriculum and instruction as those in the control group.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/Gen2Doc.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "print(\"\\nFilled Prompt:\")\n",
    "print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4058b0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Therefore, the hypothesis is supported.\n",
      "Specifically, the defect rate was reduced by 62.5%, which is well above the 20% improvement threshold stated in the hypothesis.\n",
      "○\tAlternative hypothesis (H1): There is a significant difference in defect rates between Technique A and Technique B.\n",
      "○\tNull hypothesis (H0): There is no significant difference in defect rates between Technique A and Technique B.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Therefore, the hypothesis is supported. Specifically, the defect rate was reduced by 62.5%, which is well above the 20% improvement threshold stated in the hypothesis. ○\tAlternative hypothesis (H1): There is a significant difference in defect rates between Technique A and Technique B. ○\tNull hypothesis (H0): There is no significant difference in defect rates between Technique A and Technique B.\n",
      "\n",
      "Filled Prompt:\n",
      "\n",
      "    Here is an example of a text with its corresponding hypothesis:\n",
      "\n",
      "    **Example Text:**\n",
      "    Participant Selection:\n",
      "\n",
      "Recruit 100 adults aged 25-50.\n",
      "Split the participants into two groups:\n",
      "Experimental group: Engages in 30 minutes of aerobic exercise (e.g., running or cycling) 5 times a week for 12 weeks.\n",
      "Control group: No regular exercise during the same period.\n",
      "Cognitive Function Tests:\n",
      "\n",
      "Measure participants' baseline cognitive abilities using a standardized battery of cognitive tests (focusing on memory recall, problem-solving, and executive control).\n",
      "Administer the same tests at intervals: before starting the regimen, at 6 weeks, and at the end of 12 weeks.\n",
      "Physiological Measurements:\n",
      "\n",
      "Track physical health improvements (heart rate, VO2 max) in the exercise group to ensure that aerobic capacity is improving alongside cognitive measures.\n",
      "\n",
      "\n",
      "Regular aerobic exercise enhances cognitive function, especially in tasks involving memory and executive control, in adults between the ages of 25 and 50.\n",
      "\n",
      "Cognitive Test Scores:\n",
      "\n",
      "Over the 12-week period, participants in the experimental group show an improvement in memory recall and executive function tasks compared to their baseline scores.\n",
      "The control group shows little to no improvement or a slight decline in cognitive function over time.\n",
      "Physiological Improvement:\n",
      "\n",
      "The experimental group shows increased aerobic capacity (measured through VO2 max) and lower resting heart rates, indicating enhanced physical fitness.\n",
      "Qualitative Feedback:\n",
      "\n",
      "Some participants in the experimental group report feeling more alert and focused in daily tasks, while control group participants report no significant changes.\n",
      "\n",
      "The experimental group exhibits a statistically significant improvement in cognitive function compared to the control group. The improvements are particularly noticeable in memory-related tasks and tasks requiring executive control (like planning and problem-solving).\n",
      "The correlation between improved physical fitness and cognitive enhancement supports the idea that aerobic exercise boosts brain function.\n",
      "\n",
      "The experiment supports the hypothesis that regular aerobic exercise improves cognitive function in adults aged 25-50. The increase in cognitive abilities, particularly in memory and executive control, can be attributed to the positive effects of physical activity on brain health. Hence, aerobic exercise should be considered a beneficial intervention for enhancing cognitive performance in adults.\n",
      "    \n",
      "    **Example Hypothesis:**\n",
      "    Engaging in regular aerobic exercise improves cognitive function, particularly in tasks related to memory and executive control, in adults aged 25-50.\n",
      "\n",
      "\n",
      "    \n",
      "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
      "    \n",
      "    **Definition of Hypothesis:**\n",
      "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
      "\n",
      "    **Relevant Text:**\n",
      "    Therefore, the hypothesis is supported. Specifically, the defect rate was reduced by 62.5%, which is well above the 20% improvement threshold stated in the hypothesis. ○\tAlternative hypothesis (H1): There is a significant difference in defect rates between Technique A and Technique B. ○\tNull hypothesis (H0): There is no significant difference in defect rates between Technique A and Technique B.\n",
      "\n",
      "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
      "    Therefore, the hypothesis is supported. Specifically, the defect rate was reduced by 62.5%, which is well above the 20% improvement threshold stated in the hypothesis. ○\tAlternative hypothesis (H1): There is a significant difference in defect rates between Technique A and Technique B. ○\tNull hypothesis (H0): There is no significant difference in defect rates between Technique A and Technique B.\n",
      "    \n",
      "    **Hypothesis:**\n",
      "    \n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA12A8310>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001DDA12A8310>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA12A8310>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(filled_prompt[:])  \n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA12A8310>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/GenDoc.txt\"\n",
    "example_hypothesis_path = \"Data/GenHypo.txt\"\n",
    "new_text_path = \"Data/Dummy.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Please extract the hypothesis from the relevant text. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "print(\"\\nFilled Prompt:\")\n",
    "print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de927feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Secretion of cytokines from PBMCs following RBC treatment.\n",
      "Haemolysis during sample preparation alters microRNA content of plasma.\n",
      "Tese results support the hypothesis that RBCs play a key role in modulating the immune system.\n",
      "Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "Filled Prompt:\n",
      "\n",
      "    \n",
      "    Here is an example of a text with its corresponding hypothesis:\n",
      "\n",
      "    **Example Text:**\n",
      "    Design an experiment involving 200 high school students, randomly assigned to two groups: one with AI-driven personalized learning tools and the other with traditional instructional methods. Both groups will undergo the same curriculum but with different learning supports.\n",
      "\n",
      "The AI group will receive personalized learning recommendations based on their performance, learning style, and pace, while the control group will follow standard instruction without personalized suggestions. Pre- and post-intervention academic performance in mathematics will be measured using standardized tests, and engagement will be assessed through surveys and classroom observation.\n",
      "\n",
      "Over the course of a semester, the AI group demonstrates a marked improvement in test scores compared to the control group. Engagement surveys show higher levels of interest and motivation among students in the AI group. Observations reveal that students using AI tools actively participate more in class discussions and complete assignments with greater enthusiasm.\n",
      "\n",
      "The AI-driven personalized learning approach leads to significantly better academic outcomes and higher engagement levels compared to traditional methods. This supports the conclusion that AI can effectively enhance student learning experiences and outcomes in high school mathematics.\n",
      "    \n",
      "    **Example Hypothesis:**\n",
      "    Using artificial intelligence (AI) to provide personalized learning recommendations increases student engagement and improves academic performance in high school mathematics.\n",
      "    \n",
      "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
      "    \n",
      "    \n",
      "    \n",
      "    **Definition of Hypothesis:**\n",
      "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
      "\n",
      "    **Relevant Text:**\n",
      "    Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "    **Please extract the hypothesis from the relevant text provided before and after this sentence. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
      "    Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "    \n",
      "    **Hypothesis:**\n",
      "    \n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA124ED90>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001DDA124ED90>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA124ED90>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(filled_prompt[:])  \n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA124ED90>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    \n",
    "    Here is an example of a text with its corresponding hypothesis:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    the provided above is only an example for you to understand the way hypothesis extracted from text\n",
    "    \n",
    "    \n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Please extract the hypothesis from the relevant text provided before and after this sentence. The hypothesis should be a clear, concise statement about the potential explanation or prediction derived from the text.**\n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "print(\"\\nFilled Prompt:\")\n",
    "print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad4e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Secretion of cytokines from PBMCs following RBC treatment.\n",
      "Haemolysis during sample preparation alters microRNA content of plasma.\n",
      "Tese results support the hypothesis that RBCs play a key role in modulating the immune system.\n",
      "Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA1E19350>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001DDA1E19350>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA1E19350>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 104\u001b[0m\n\u001b[0;32m     97\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mrelevant_text)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Debug: Check filled prompt\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m#print(\"\\nFilled Prompt:\")\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#print(filled_prompt[:])  \u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DDA1E19350>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    \n",
    "    Here is an example of a text with its corresponding hypothesis and experiment:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "    \n",
    "    **Definition of Experiment:**\n",
    "    An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether \n",
    "    a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are \n",
    "    kept constant, allowing researchers to observe and measure the effects of the changes. The goal is to gather \n",
    "    data that either supports or refutes the hypothesis.\n",
    "    \n",
    "    the explanation provided so far is only an example for you to understand the way hypothesis extracted from text.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Extract the hypothesis and experiment from the relevant text provided before and after this sentence.**\n",
    "    **provide the Hypothesis and Experiment separately** based on the text with title Relevant Text extract the experiment and \n",
    "    hypothesis.\n",
    "    \n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \n",
    "    \n",
    "    **Experiment:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "#print(\"\\nFilled Prompt:\")\n",
    "#print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cf29db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Secretion of cytokines from PBMCs following RBC treatment.\n",
      "Haemolysis during sample preparation alters microRNA content of plasma.\n",
      "Tese results support the hypothesis that RBCs play a key role in modulating the immune system.\n",
      "Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Secretion of cytokines from PBMCs following RBC treatment. Haemolysis during sample preparation alters microRNA content of plasma. Tese results support the hypothesis that RBCs play a key role in modulating the immune system. Te PBMCs were then washed once in PBS before commencing staining with the relevant monoclonal antibodies.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=10434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000237EC2F9110>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000237EC2F9110>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=10434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000237EC2F9110>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m\n\u001b[0;32m     97\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mrelevant_text)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Debug: Check filled prompt\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m#print(\"\\nFilled Prompt:\")\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#print(filled_prompt[:])  \u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=10434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000237EC2F9110>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:10434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "documents = [Document(page_content=chunk, metadata={\"id\": str(i)}) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = TFIDFRetriever.from_documents(documents)\n",
    "\n",
    "# Retrieve relevant chunks from the new text\n",
    "query = \"Extract relevant text for hypothesis extraction from the following document content.\"\n",
    "relevant_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)\n",
    "\n",
    "relevant_text = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])  \n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    the text with the corresponding hypothesis and experiment is just an example.\n",
    "    Here is an example of a text with its corresponding hypothesis and experiment:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A hypothesis is a testable statement about a possible explanation or prediction.\n",
    "    \n",
    "    **Definition of Experiment:**\n",
    "    An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether \n",
    "    a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are \n",
    "    kept constant, allowing researchers to observe and measure the effects of the changes. The goal is to gather \n",
    "    data that either supports or refutes the hypothesis.\n",
    "    \n",
    "    the explanation provided so far is only an example for you to understand the way hypothesis extracted from text.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Extract the hypothesis and experiment from the relevant text provided before and after this sentence.**\n",
    "    **provide the Hypothesis and Experiment separately** based on the text with title Relevant Text extract the experiment and \n",
    "    hypothesis.\n",
    "    \n",
    "    {relevant_text}\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    \n",
    "    \n",
    "    **Experiment:**\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt\n",
    "#print(\"\\nFilled Prompt:\")\n",
    "#print(filled_prompt[:])  \n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e70a78",
   "metadata": {},
   "source": [
    "### Date : 2024/09/15\n",
    "#### Time : 14:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea68a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Protein detection.\n",
      "E.K., B.H., S.M., and S.C. designed the research and edited the paper.\n",
      "performed experiments; E.K.\n",
      "In this study, two multiplex assays were utilised.\n",
      "Data are statistically signifcantly diferent if\n",
      "*p < 0.05, **p < 0.01, or ***p < 0.001.\n",
      "Comparison of multiple treatment groups were statistically evaluated using a one-way ANOVA with correction for multiple comparisons to assess statistical signifcance.\n",
      "Te assays were run on the Luminex 200 system (Bio-Rad) and fuorescence values were collected.\n",
      "Graphing of results was performed using GraphPad Prism sofware (ver.\n",
      "Tus, further understanding of the biology of the condition may be valuable in identify- ing novel therapeutic targets or improving early detection in at-risk groups.\n",
      "Statistical analysis.\n",
      "\n",
      "Combined Relevant Text:\n",
      "Protein detection. E.K., B.H., S.M., and S.C. designed the research and edited the paper. performed experiments; E.K. In this study, two multiplex assays were utilised. Data are statistically signifcantly diferent if\n",
      "*p < 0.05, **p < 0.01, or ***p < 0.001. Comparison of multiple treatment groups were statistically evaluated using a one-way ANOVA with correction for multiple comparisons to assess statistical signifcance. Te assays were run on the Luminex 200 system (Bio-Rad) and fuorescence values were collected. Graphing of results was performed using GraphPad Prism sofware (ver. Tus, further understanding of the biology of the condition may be valuable in identify- ing novel therapeutic targets or improving early detection in at-risk groups. Statistical analysis.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DD9F0BAA10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001DD9F0BAA10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DD9F0BAA10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 120\u001b[0m\n\u001b[0;32m    113\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mrelevant_text)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Debug: Check filled prompt (optional)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m#print(\"\\nFilled Prompt:\")\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#print(filled_prompt[:])\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Invoke the model with the filled prompt\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Print the generated hypothesis and experiment\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001DD9F0BAA10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize BERT (or Sentence-BERT) for sentence embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"Extract relevant text for hypothesis and experiment  extraction from the following document content.\n",
    "\n",
    "    **Definition of Hypothesis:**\n",
    "    A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "    \n",
    "    **Definition of Experiment:**\n",
    "    An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether \n",
    "    a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are \n",
    "    kept constant, allowing researchers to observe and measure the effects of the changes. The goal is to gather \n",
    "    data that either supports or refutes the hypothesis.\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = util.pytorch_cos_sim(query_embedding, sentence_embeddings)\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.topk(k=top_k)[1].tolist()[0]\n",
    "\n",
    "# Retrieve the relevant chunks\n",
    "relevant_chunks = [chunks[i] for i in top_k_indices]\n",
    "\n",
    "# Debug: Check retrieved chunks\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk)\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Debug: Check combined relevant text\n",
    "print(\"\\nCombined Relevant Text:\")\n",
    "print(relevant_text[:])\n",
    "\n",
    "# Prepare the prompt with the example and retrieved relevant text\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    The text with the corresponding hypothesis and experiment is just an example.\n",
    "    Here is an example of a text with its corresponding hypothesis and experiment:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "    \n",
    "    **Definition of Experiment:**\n",
    "    An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether \n",
    "    a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are \n",
    "    kept constant, allowing researchers to observe and measure the effects of the changes. The goal is to gather \n",
    "    data that either supports or refutes the hypothesis.\n",
    "    \n",
    "    The explanation provided so far is only an example for you to understand the way hypotheses are extracted from text.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Extract the hypothesis and experiment from the relevant text provided before and after this sentence.**\n",
    "    **Provide the Hypothesis and Experiment separately** based on the text with title 'Relevant Text' extracted from the experiment and hypothesis.\n",
    "\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Debug: Check filled prompt (optional)\n",
    "#print(\"\\nFilled Prompt:\")\n",
    "#print(filled_prompt[:])\n",
    "\n",
    "# Invoke the model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "# Print the generated hypothesis and experiment\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3838ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000166B86F2A10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000166B86F2A10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000166B86F2A10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mrelevant_text)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Invoke the TinyLlama model with the filled prompt\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(filled_prompt)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Print the generated hypothesis and experiment\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:250\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    249\u001b[0m     }\n\u001b[1;32m--> 250\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    251\u001b[0m     url\u001b[38;5;241m=\u001b[39mapi_url,\n\u001b[0;32m    252\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    255\u001b[0m     },\n\u001b[0;32m    256\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth,\n\u001b[0;32m    257\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest_payload,\n\u001b[0;32m    258\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='notebooks.weburban.com', port=12434): Max retries exceeded with url: /api/generate (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000166B86F2A10>: Failed to resolve 'notebooks.weburban.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize the LLM (TinyLlama)\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a better BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis and experiment extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    The following is an example of a text with its corresponding hypothesis and experiment:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "    \n",
    "    **Definition of Experiment:**\n",
    "    An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false.\n",
    "\n",
    "    Now, please read the following **Relevant Text** and extract the hypothesis and experiment:\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Step-by-step reasoning**:\n",
    "    1. Identify statements that propose a relationship or expected result (hypothesis).\n",
    "    2. Identify statements that explain testing or validation steps (experiment).\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Describe the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a detailed experimental procedure in 2-3 sentences.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Invoke the TinyLlama model with the filled prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "# Print the generated hypothesis and experiment\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850b65b",
   "metadata": {},
   "source": [
    "### Date : 2024/09/15\n",
    "#### Time : 14:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd71078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'name': 'flanT5:latest', 'model': 'flanT5:latest', 'modified_at': '2024-10-18T09:30:47.189735154Z', 'size': 496288729, 'digest': '7a929ef5596adbc4f8a090da61da9c23e02c8cacce0c547059ddc9765cda8eb3', 'details': {'parent_model': '', 'format': 'gguf', 'family': 't5', 'families': ['t5'], 'parameter_size': '247.58M', 'quantization_level': 'F16'}}, {'name': 'flanT5_Q8:latest', 'model': 'flanT5_Q8:latest', 'modified_at': '2024-10-18T01:48:56.267189427Z', 'size': 310495610, 'digest': 'f3d524247f7e302770f085c1e52f0b913ab336d5a1c75edcafe654b3b8625120', 'details': {'parent_model': '', 'format': 'gguf', 'family': 't5', 'families': ['t5'], 'parameter_size': '247.58M', 'quantization_level': 'Q8_0'}}, {'name': 'phi3.5:latest', 'model': 'phi3.5:latest', 'modified_at': '2024-10-18T01:41:23.843218919Z', 'size': 2176178843, 'digest': '61819fb370a3c1a9be6694869331e5f85f867a079e9271d66cb223acb81d04ba', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '3.8B', 'quantization_level': 'Q4_0'}}, {'name': 'phi:latest', 'model': 'phi:latest', 'modified_at': '2024-10-18T01:40:36.235231383Z', 'size': 1602463378, 'digest': 'e2fd6321a5fe6bb3ac8a4e6f1cf04477fd2dea2924cf53237a995387e152ee9c', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi2', 'families': ['phi2'], 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma2:2b', 'model': 'gemma2:2b', 'modified_at': '2024-10-18T01:39:44.799247654Z', 'size': 1629518495, 'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma2', 'families': ['gemma2'], 'parameter_size': '2.6B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma:2b', 'model': 'gemma:2b', 'modified_at': '2024-10-18T01:39:10.735260179Z', 'size': 1678456656, 'digest': 'b50d6c999e592ae4f79acae23b4feaefbdfceaa7cd366df2610e3072c052a160', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma', 'families': ['gemma'], 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'stablelm2:latest', 'model': 'stablelm2:latest', 'modified_at': '2024-10-18T01:38:33.107275771Z', 'size': 982790462, 'digest': '714a6116cffa8b415b52c62a7a2d09ba6227ed733baa0025c937a36aee5504f3', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'stablelm', 'families': ['stablelm'], 'parameter_size': '2B', 'quantization_level': 'Q4_0'}}, {'name': 'orca-mini:latest', 'model': 'orca-mini:latest', 'modified_at': '2024-10-18T01:38:06.511287973Z', 'size': 1979947443, 'digest': '2dbd9f439647093cf773c325b0b3081a11f1b1426d61dee8b946f8f6555a1755', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': None, 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'tinydolphin:latest', 'model': 'tinydolphin:latest', 'modified_at': '2024-10-18T01:37:23.151310125Z', 'size': 636743607, 'digest': '0f9dd11f824c7f9a881c9e663d71c1bb0ed0b1d76dd21f6c679f7193c3be7308', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '1B', 'quantization_level': 'Q4_0'}}, {'name': 'tinyllama:latest', 'model': 'tinyllama:latest', 'modified_at': '2024-10-18T01:37:01.427322338Z', 'size': 637700138, 'digest': '2644915ede352ea7bdfaff0bfac0be74c719d5d5202acb63a6fb095b52f394a4', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '1B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:1.5b', 'model': 'qwen2:1.5b', 'modified_at': '2024-10-18T01:36:38.611336019Z', 'size': 934964102, 'digest': 'f6daf2b25194025ae2d5288f2afd041997ce48116807a3b612c1a96b09bec03a', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '1.5B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:0.5b', 'model': 'qwen2:0.5b', 'modified_at': '2024-10-18T01:36:17.955349187Z', 'size': 352164041, 'digest': '6f48b936a09f7743c7dd30e72fdb14cba296bc5861902e4d0c387e8fb5050b39', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '494.03M', 'quantization_level': 'Q4_0'}}, {'name': 'qwen:1.8b', 'model': 'qwen:1.8b', 'modified_at': '2024-10-18T01:36:17.071349768Z', 'size': 1120243281, 'digest': 'b6e8ec2e7126ea21d1817e28ad69a2bebdd5547a9af223fbb927054dc66fc4ce', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '2B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen:0.5b', 'model': 'qwen:0.5b', 'modified_at': '2024-10-18T01:35:52.631366389Z', 'size': 394998579, 'digest': 'b5dc5e784f2a3ee1582373093acf69a2f4e2ac1710b253a001712b86a61f88bb', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '620M', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2.5:0.5b', 'model': 'qwen2.5:0.5b', 'modified_at': '2024-10-17T06:40:37.514909596Z', 'size': 397821319, 'digest': 'a8b0c51577010a279d933d14c2a8ab4b268079d44c5c8830c0a93900f1827c67', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '494.03M', 'quantization_level': 'Q4_K_M'}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text:**\n",
      "\n",
      "The experiment involves 200 high school students split into two groups. One group uses AI-driven personalized learning tools, while the other follows traditional methods. Both complete the same math curriculum over a semester and receive standardized tests at the end of the course. Engagement surveys are also given to measure student interest and motivation.\n",
      "\n",
      "**Step-by-step reasoning:**\n",
      "\n",
      "1. **Identify statements that propose a relationship or expected result (hypothesis):**\n",
      "   - **Hypothesis:** Using AI to provide personalized learning recommendations leads to significant academic outcomes and higher engagement levels.\n",
      "   \n",
      "2. **Identify statements that explain testing or validation steps (experiment):**\n",
      "   - **Experiment 1:**\n",
      "     - The experiment involves 200 high school students split into two groups: one with AI-driven personalized learning tools and the other with traditional instructional methods.\n",
      "     - Both groups will undergo the same curriculum but with different learning supports. Over a semester, academic performance in mathematics is measured using standardized tests and engagement levels are assessed through surveys and classroom observation.\n",
      "\n",
      "The example provided does not include the hypothesis or experiment as stated in the original question. However, based on the example text and its structure, it seems to be an experimental design where students receive personalized AI learning tools versus traditional instructional methods. If you have a specific example of an experimental design, please provide more details for me to help understand the setup and hypothesis behind it.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Use list to see available models\n",
    "print(client.list())\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis and experiment extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    The following is an example of a text with its corresponding hypothesis and experiment:\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    **Definition of Hypothesis:**\n",
    "    A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "    \n",
    "    **Definition of Experiment:**\n",
    "    An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false.\n",
    "\n",
    "    Now, please read the following **Relevant Text** and extract the hypothesis and experiment:\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Step-by-step reasoning**:\n",
    "    1. Identify statements that propose a relationship or expected result (hypothesis).\n",
    "    2. Identify statements that explain testing or validation steps (experiment).\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Describe the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a detailed experimental procedure in 2-3 sentences.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis and experiment\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484be158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'name': 'flanT5:latest', 'model': 'flanT5:latest', 'modified_at': '2024-10-18T09:30:47.189735154Z', 'size': 496288729, 'digest': '7a929ef5596adbc4f8a090da61da9c23e02c8cacce0c547059ddc9765cda8eb3', 'details': {'parent_model': '', 'format': 'gguf', 'family': 't5', 'families': ['t5'], 'parameter_size': '247.58M', 'quantization_level': 'F16'}}, {'name': 'flanT5_Q8:latest', 'model': 'flanT5_Q8:latest', 'modified_at': '2024-10-18T01:48:56.267189427Z', 'size': 310495610, 'digest': 'f3d524247f7e302770f085c1e52f0b913ab336d5a1c75edcafe654b3b8625120', 'details': {'parent_model': '', 'format': 'gguf', 'family': 't5', 'families': ['t5'], 'parameter_size': '247.58M', 'quantization_level': 'Q8_0'}}, {'name': 'phi3.5:latest', 'model': 'phi3.5:latest', 'modified_at': '2024-10-18T01:41:23.843218919Z', 'size': 2176178843, 'digest': '61819fb370a3c1a9be6694869331e5f85f867a079e9271d66cb223acb81d04ba', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '3.8B', 'quantization_level': 'Q4_0'}}, {'name': 'phi:latest', 'model': 'phi:latest', 'modified_at': '2024-10-18T01:40:36.235231383Z', 'size': 1602463378, 'digest': 'e2fd6321a5fe6bb3ac8a4e6f1cf04477fd2dea2924cf53237a995387e152ee9c', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi2', 'families': ['phi2'], 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma2:2b', 'model': 'gemma2:2b', 'modified_at': '2024-10-18T01:39:44.799247654Z', 'size': 1629518495, 'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma2', 'families': ['gemma2'], 'parameter_size': '2.6B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma:2b', 'model': 'gemma:2b', 'modified_at': '2024-10-18T01:39:10.735260179Z', 'size': 1678456656, 'digest': 'b50d6c999e592ae4f79acae23b4feaefbdfceaa7cd366df2610e3072c052a160', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma', 'families': ['gemma'], 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'stablelm2:latest', 'model': 'stablelm2:latest', 'modified_at': '2024-10-18T01:38:33.107275771Z', 'size': 982790462, 'digest': '714a6116cffa8b415b52c62a7a2d09ba6227ed733baa0025c937a36aee5504f3', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'stablelm', 'families': ['stablelm'], 'parameter_size': '2B', 'quantization_level': 'Q4_0'}}, {'name': 'orca-mini:latest', 'model': 'orca-mini:latest', 'modified_at': '2024-10-18T01:38:06.511287973Z', 'size': 1979947443, 'digest': '2dbd9f439647093cf773c325b0b3081a11f1b1426d61dee8b946f8f6555a1755', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': None, 'parameter_size': '3B', 'quantization_level': 'Q4_0'}}, {'name': 'tinydolphin:latest', 'model': 'tinydolphin:latest', 'modified_at': '2024-10-18T01:37:23.151310125Z', 'size': 636743607, 'digest': '0f9dd11f824c7f9a881c9e663d71c1bb0ed0b1d76dd21f6c679f7193c3be7308', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '1B', 'quantization_level': 'Q4_0'}}, {'name': 'tinyllama:latest', 'model': 'tinyllama:latest', 'modified_at': '2024-10-18T01:37:01.427322338Z', 'size': 637700138, 'digest': '2644915ede352ea7bdfaff0bfac0be74c719d5d5202acb63a6fb095b52f394a4', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '1B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:1.5b', 'model': 'qwen2:1.5b', 'modified_at': '2024-10-18T01:36:38.611336019Z', 'size': 934964102, 'digest': 'f6daf2b25194025ae2d5288f2afd041997ce48116807a3b612c1a96b09bec03a', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '1.5B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen:1.8b', 'model': 'qwen:1.8b', 'modified_at': '2024-10-18T01:36:17.071349768Z', 'size': 1120243281, 'digest': 'b6e8ec2e7126ea21d1817e28ad69a2bebdd5547a9af223fbb927054dc66fc4ce', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '2B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:0.5b', 'model': 'qwen2:0.5b', 'modified_at': '2024-10-18T01:36:17.955349187Z', 'size': 352164041, 'digest': '6f48b936a09f7743c7dd30e72fdb14cba296bc5861902e4d0c387e8fb5050b39', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '494.03M', 'quantization_level': 'Q4_0'}}, {'name': 'qwen:0.5b', 'model': 'qwen:0.5b', 'modified_at': '2024-10-18T01:35:52.631366389Z', 'size': 394998579, 'digest': 'b5dc5e784f2a3ee1582373093acf69a2f4e2ac1710b253a001712b86a61f88bb', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '620M', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2.5:0.5b', 'model': 'qwen2.5:0.5b', 'modified_at': '2024-10-17T06:40:37.514909596Z', 'size': 397821319, 'digest': 'a8b0c51577010a279d933d14c2a8ab4b268079d44c5c8830c0a93900f1827c67', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '494.03M', 'quantization_level': 'Q4_K_M'}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Hypothesis:** AI-driven personalized learning tools significantly improve student engagement and academic performance in high school mathematics.\n",
      "\n",
      "**Experiment:** \n",
      "1. The experiment involves 200 high school students split into two groups, one using AI-driven personalized learning tools (AI group) and the other following traditional methods (control group).\n",
      "2. Both groups complete the same math curriculum over a semester.\n",
      "3. Academic performance is measured with tests before and after.\n",
      "\n",
      "**Observation:**\n",
      "1. The AI group demonstrates an improvement in test scores compared to the control group, as evidenced by higher engagement surveys and more active participation in class discussions among students using AI tools.\n",
      "2. Students in the AI group participate more actively in discussions and complete assignments with greater enthusiasm during the semester, which is consistent with their improved academic performance.\n",
      "\n",
      "**Evaluation:**\n",
      "1. The observed data supports the hypothesis that AI-driven personalized learning enhances both engagement and academic performance in high school mathematics by demonstrating statistically significant improvements in test scores and increased motivation among students using AI tools.\n",
      "2. This evidence suggests that incorporating AI into educational settings can lead to better outcomes for students, with potential applications across multiple subject areas.\n",
      "\n",
      "**Logical Conclusions:**\n",
      "1. The results indicate that AI-driven personalized learning improves academic outcomes and student engagement compared to traditional methods.\n",
      "2. These findings validate the effectiveness of AI in enhancing educational experiences and should be considered in broader contexts like teaching and learning practices.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Use list to see available models\n",
    "print(client.list())\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    The following is an example of a text with its corresponding elements: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Text:**\n",
    "    {example_text}\n",
    "    \n",
    "    **Example Hypothesis:**\n",
    "    {example_hypothesis}\n",
    "    \n",
    "    **Example Experiment:**\n",
    "    {example_experiment}\n",
    "    \n",
    "    **Example Observation:**\n",
    "    {example_observation}\n",
    "    \n",
    "    **Example Evaluation:**\n",
    "    {example_evaluation}\n",
    "    \n",
    "    **Example Logical Conclusions:**\n",
    "    {example_conclusion}\n",
    "    \n",
    "    **Definitions**:\n",
    "    - **Hypothesis**: A concise statement about the expected result of an experiment or project.\n",
    "    - **Experiment**: A structured and controlled procedure to test the hypothesis.\n",
    "    - **Observation**: Data or information collected during the experiment.\n",
    "    - **Evaluation**: An assessment of the results, discussing whether they support the hypothesis.\n",
    "    - **Logical Conclusions**: Deductions based on the evaluation and analysis, forming the final interpretation.\n",
    "\n",
    "    Now, please read the following **Relevant Text** and extract the hypothesis, experiment, observation, evaluation, and logical conclusions:\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Step-by-step reasoning**:\n",
    "    1. Identify statements that propose a relationship or expected result (hypothesis).\n",
    "    2. Identify statements that explain testing or validation steps (experiment).\n",
    "    3. Look for data or information collected during the experiment (observation).\n",
    "    4. Assess statements analyzing the findings (evaluation).\n",
    "    5. Identify conclusions drawn based on evaluation (logical conclusions).\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Describe the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a detailed experimental procedure in 2-3 sentences.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment in 1-2 sentences.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation in 1-2 sentences, discussing support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the logical conclusion in 1-2 sentences.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49d1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text (Hypothesis)**:\n",
      "6, USA). Statistical analysis. Comparison of multiple treatment groups were statistically evaluated using a one-way ANOVA with correction for multiple comparisons to assess statistical signifcance.\n",
      "\n",
      "The hypothesis can be summarized as follows: Statistical analysis was performed using a one-way ANOVA to compare multiple treatment groups and evaluate their significance in statistical terms.\n",
      "\n",
      "**Experiment (Evaluation)**:\n",
      "Data from the experiment were collected, analyzed, and evaluated statistically. A sample of PBMCs was used for fow cytometry analysis. Results indicated that both the PHA-P and PHA+ccRBC treated groups differed significantly from PBMCs at multiple points (p < 0.05, **p < 0.01, or ***p < 0.001). These findings suggest potential benefits of specific treatments on certain populations.\n",
      "\n",
      "**Logical Conclusions (Evaluation)**:\n",
      "The evaluation confirms that the experimental results demonstrate significant differences between treatment groups and the observed statistical significance supports the hypothesis that one-way ANOVA can be used to evaluate statistical comparisons in multiple treatment groups.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the following **Relevant Text**: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Definitions** (simplified):\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Instructions**:\n",
    "    1. Use only the information in the Relevant Text to create responses.\n",
    "    2. Each answer should be concise and based solely on the content provided.\n",
    "    3. Avoid mirroring any example phrases directly; describe observations, evaluations, and conclusions uniquely.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88134793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "Sure! Let's go through each element in the provided text:\n",
      "\n",
      "**Hypothesis:** Identify the hypothesis in 1-2 concise sentences.\n",
      "\n",
      "**Experiment:** Provide a short description of the experiment procedure.\n",
      "\n",
      "**Observation:** Summarize key observations from the experiment.\n",
      "\n",
      "**Evaluation:** Summarize the evaluation, including support for the hypothesis.\n",
      "\n",
      "**Logical Conclusions:** State the final interpretation based on the evaluation.\n",
      "\n",
      "Here's how we can extract these elements:\n",
      "\n",
      "1. **Hypothesis:**\n",
      "   - The Hypothesis provides a clear statement about what is to be identified in the text.\n",
      "   \n",
      "2. **Experiment:**\n",
      "   - The Experiment section describes the specific steps or processes that were followed during the experiment.\n",
      "\n",
      "3. **Observation:**\n",
      "   - Observations are described in detail, providing concrete details of the data gathered.\n",
      "\n",
      "4. **Evaluation:**\n",
      "   - Evaluation summarizes the overall results and how well they support the hypothesis.\n",
      "   \n",
      "5. **Logical Conclusions:**\n",
      "   - The Logical Conclusions draw a conclusion based on the evaluation of the observations.\n",
      "\n",
      "So, the extraction process is:\n",
      "\n",
      "1. Identify the hypothesis in 1-2 concise sentences.\n",
      "2. Provide a short description of the experiment procedure.\n",
      "3. Summarize key observations from the experiment.\n",
      "4. Summarize the evaluation, including support for the hypothesis.\n",
      "5. State the final interpretation based on the evaluation.\n",
      "\n",
      "I'll now apply this process to the given text:\n",
      "\n",
      "**Hypothesis:**\n",
      "- Identify the hypothesis in 1-2 concise sentences.\n",
      "\n",
      "The provided text does not include a clear hypothesis or any specific steps described in an experiment procedure, so I will skip that part and focus solely on the observations and evaluations mentioned.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the text without printing the **Relevant Text** itself. Make sure to address each element (Hypothesis, Experiment, Observation, Evaluation, and Logical Conclusions) based on the entire provided text context.\n",
    "\n",
    "    **Example Definitions**:\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Instructions**:\n",
    "    - Use the entire text context to provide responses for all elements.\n",
    "    - Each answer should be unique and focus only on the extracted content, not examples or phrasing.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \n",
    "    make sure to extract these elements from the relevant text.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c8acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "1. 2 sentences.\n",
      "2. Procedure: A detailed set of steps or procedures that guide the task.\n",
      "3. Observation: An observation made during the execution of the procedure to assess its outcomes.\n",
      "4. Evaluation: A summary of the results obtained from the observed process and their implications on the hypothesis.\n",
      "5. Logical Conclusions: A final interpretation based on the evaluation, suggesting what can be concluded or drawn from the findings.\n",
      "\n",
      "Note that in this context, \"Relevant Text\" is hidden; you would need to provide a specific text for it to proceed with analysis. Please provide a relevant piece of text for extraction.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with stricter instructions to avoid repetition of instructions\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, extract each element from the entire **Relevant Text** without printing or rephrasing the definitions or instructions.\n",
    "\n",
    "    **Relevant Text is hidden** - Use only the provided context in the Relevant Text to respond directly.\n",
    "\n",
    "    **Directly output the following extracted elements based on the relevant text**:\n",
    "    \n",
    "    - **Hypothesis:** 1-2 sentences.\n",
    "    - **Experiment:** Short description of the procedure.\n",
    "    - **Observation:** Summary of key observations.\n",
    "    - **Evaluation:** Summary of results and support for hypothesis.\n",
    "    - **Logical Conclusions:** Final interpretation based on evaluation.\n",
    "    \n",
    "    Do not include any explanatory or instructional text, definitions, or reiterations. Only provide the extracted content.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c270a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "- **Hypothesis:** The hypothesis states that we are interested in understanding the effects of exposure to a certain substance on various physiological processes and outcomes.\n",
      "\n",
      "- **Experiment:** An experiment is conducted by measuring the effect of exposing participants to different concentrations of the substance being studied. These measurements can then be analyzed to identify patterns or correlations between exposure levels and observed outcomes.\n",
      "\n",
      "- **Observation:** Observations are made during the experiment phase, where researchers record data points from both the treated group (the subjects exposed to the substance) and a control group that receives no intervention. This information is used to infer potential effects of the substance on different physiological processes.\n",
      "\n",
      "- **Evaluation:** The evaluation stage involves comparing the observed outcomes with expected values through statistical analysis or other methods, to determine if the hypothesis can be supported or contradicted based on the data collected during the experiment.\n",
      "\n",
      "- **Logical Conclusions:** Logical conclusions are drawn from the results of the experiment. These conclusions are derived from the experimental design and interpretation of the data obtained. For example, if it is found that the substance has a significant positive impact on one physiological process but not another due to differences in the study's population or variables considered, then these findings could be used as evidence supporting the hypothesis that exposure to this substance leads to specific effects on different bodily functions.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Create a prompt that requires direct extraction with no additional context or instruction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Extract the following elements from the provided text:\n",
    "\n",
    "    - **Hypothesis**\n",
    "    - **Experiment**\n",
    "    - **Observation**\n",
    "    - **Evaluation**\n",
    "    - **Logical Conclusions**\n",
    "\n",
    "    Directly provide the content for each element.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8d157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Hypothesis**: I will perform an experiment to measure the impact of environmental changes on plant growth.\n",
      "\n",
      "- **Experiment**: I will grow a variety of plants in controlled environments with different levels of environmental changes, such as temperature fluctuations or increased water availability.\n",
      "\n",
      "- **Observation**: After several weeks, I will measure the height and overall health of each plant. I will record the changes in soil moisture level and nutrient content.\n",
      "\n",
      "- **Evaluation**: Based on the observations, I will compare the growth patterns of plants with varying environmental conditions to determine which variables have a greater impact on plant growth.\n",
      "\n",
      "- **Logical Conclusions**: I expect that plants grown under increased water availability will grow taller than those under controlled environments with lower water levels. Additionally, I predict that plants growing in areas with higher nutrient content may outgrow their peers due to better nutrition and increased root growth.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Create a prompt that requires direct extraction with no additional context or instruction\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Extract the following elements from the provided text:\n",
    "\n",
    "    - **Hypothesis**\n",
    "    - **Experiment**\n",
    "    - **Observation**\n",
    "    - **Evaluation**\n",
    "    - **Logical Conclusions**\n",
    "\n",
    "    Directly provide the content for each element.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2c3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Hypothesis:**\n",
      "The use of AI to provide personalized learning is expected to increase student engagement, improve academic performance.\n",
      "\n",
      "**Experiment:**\n",
      "The experiment involves 200 students split into two groups: one using AI-driven tools and another using traditional methods. Both groups are given pre- and post-tests on their comprehension of a specific topic.\n",
      "\n",
      "**Observation:**\n",
      "Students using AI tools showed increased engagement in class discussions and enthusiasm in assignments, which were measured by pre-test and post-test scores.\n",
      "\n",
      "**Evaluation:**\n",
      "The data supports the hypothesis that personalized learning through AI improves student performance. Specifically:\n",
      "- The pre-test showed better comprehension than before, indicating a positive impact on students' ability to understand the material.\n",
      "- The post-tests indicated an increase in engagement levels among students using AI tools compared to those using traditional methods.\n",
      "\n",
      "**Logical Conclusions:**\n",
      "The findings of this experiment suggest that AI-driven personalized learning can significantly enhance student learning outcomes across various educational contexts.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "example_text_path = \"Data/Gen2Doc.txt\"\n",
    "example_hypothesis_path = \"Data/Gen2Hypo.txt\"\n",
    "example_experiment_path = \"Data/Gen2Ex.txt\"\n",
    "example_observation_path = \"Data/Gen2Obs.txt\"\n",
    "example_evaluation_path = \"Data/Gen2Eval.txt\"\n",
    "example_conclusion_path = \"Data/Gen2Concl.txt\"\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "example_text = load_text_file(example_text_path)\n",
    "example_hypothesis = load_text_file(example_hypothesis_path)\n",
    "example_experiment = load_text_file(example_experiment_path)\n",
    "example_observation = load_text_file(example_observation_path)\n",
    "example_evaluation = load_text_file(example_evaluation_path)\n",
    "example_conclusion = load_text_file(example_conclusion_path)\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Create a prompt that includes brief examples for each element\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Extract the following elements from the provided text based on the examples given:\n",
    "\n",
    "    **Example Hypothesis:** Using AI to provide personalized learning increases student engagement and improves academic performance.\n",
    "    \n",
    "    **Example Experiment:** The experiment involves 200 students split into groups with AI-driven tools versus traditional methods, with results measured by pre- and post-tests.\n",
    "    \n",
    "    **Example Observation:** Students using AI tools showed increased engagement in class discussions and enthusiasm in assignments.\n",
    "    \n",
    "    **Example Evaluation:** The data supports the hypothesis, showing statistically significant improvements in test scores and motivation in the AI group.\n",
    "    \n",
    "    **Example Logical Conclusions:** AI-driven personalized learning appears to enhance learning outcomes, suggesting benefits across various educational contexts.\n",
    "\n",
    "    **Please Extract the Following Elements from the Relevant Text:**\n",
    "\n",
    "    - **Hypothesis:** Describe the main hypothesis.\n",
    "    - **Experiment:** Summarize the experimental setup.\n",
    "    - **Observation:** Outline the key observations.\n",
    "    - **Evaluation:** Summarize the evaluation and its support for the hypothesis.\n",
    "    - **Logical Conclusions:** State the final interpretation based on the findings.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c9c217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text:**\n",
      "\n",
      "1. Hypothesis:\n",
      "   The text does not contain any specific hypothesis.\n",
      "\n",
      "2. Experiment:\n",
      "   - **Procedure**: 6, USA). Statistical analysis. Comparison of multiple treatment groups were statistically evaluated using a one-way ANOVA with correction for multiple comparisons to assess statistical signifcance.\n",
      "   - **Methods**:\n",
      "     * Data was collected from PBMCs (Blood Cells Microscopic Examination) before and after treatment using fow cytometry.\n",
      "     * The analysis method used was one-way ANOVA with correction for multiple comparisons. To evaluate statistical significance, a p-value of < 0.05 was required.\n",
      "\n",
      "3. Observation:\n",
      "   - **Data presented**: Box plots with median concentrations were shown to illustrate the distribution of data values.\n",
      "   - **Observations**:\n",
      "     * All data points are statistically different from PBMCs treated with PHA-P if *p < 0.05, **p < 0.01, ***p < 0.001, or ****p < 0.0001 and PBMC + PHA + oRBCs are statistically different from PBMC + PHA + ccRBCs if #p < 0.05.\n",
      "     * Data points from the group treated with OMEGACOQ (one million of chloramphenicol) were more similar to those in the group treated with PBS (without antibiotics) than those in the group treated with BCG (Bacillus Calmette-Guérin), indicating a significant difference.\n",
      "\n",
      "4. Evaluation:\n",
      "   - **Statistical analysis**: The statistical methods used were appropriate for evaluating multiple treatment groups and comparing them to each other.\n",
      "   - **Multiple comparison correction**: Holm's method was correctly applied, ensuring the significance of individual comparisons within multiple groups is not affected by multiple testing.\n",
      "\n",
      "5. Logical conclusions:\n",
      "   - Based on the evaluation, there is a significant difference in data between different treatment groups (PHA-P vs. OMEGACOQ) and between PBMCs (PBMCs treated with PBS vs. PBMCs treated with BCG).\n",
      "   - The results are consistent with previous studies showing that the use of antibiotics can reduce inflammation, indicating that OMEGACOQ may have a similar effect.\n",
      "   - The observed differences in data suggest potential applications for this treatment regimen.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "\n",
    "new_text_path = \"Data/blood_cells.txt\"\n",
    "\n",
    "\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the following **Relevant Text**: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Definitions** (simplified):\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Instructions**:\n",
    "    1. Use only the information in the Relevant Text to create responses.\n",
    "    2. Each answer should be concise and based solely on the content provided.\n",
    "    3. Avoid mirroring any example phrases directly; describe observations, evaluations, and conclusions uniquely.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a49b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amirs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\amirs\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "**Relevant Text Hypothesis (50 words):**\n",
      "\n",
      "In the mining industry, vibration data is crucial for monitoring ground vibrations caused by controlled detonations in mines. The ability to monitor these vibrations can be useful for regulatory compliance but also for various non-regulatory applications.\n",
      "\n",
      "**Experiment (150-200 words):**\n",
      "1. **Procedure:**\n",
      "   - Use a cover arranged to fit the housing and allow it to be easily removed.\n",
      "   - Install a radio receiver for receiving a radio signal encoded with program data, which is then processed by the electronic circuit to operate according to the provided program data.\n",
      "\n",
      "2. **Observation (50-100 words):**\n",
      "   - The device successfully receives and processes the encoded program data transmitted via the communication channel.\n",
      "   - After processing the program data, it can be used to control the electronic circuit for collecting vibration data at a specified sample rate, recording only specific times or only collecting vibrations if they exceed certain thresholds.\n",
      "\n",
      "3. **Evaluation (50-100 words):**\n",
      "   - The device operates effectively and efficiently, enabling the collection of vibration data from controlled detonations.\n",
      "   - This demonstrates that the proposed electronic circuit successfully processes the program data to gather vibration data without compromising its functionality.\n",
      "\n",
      "4. **Logical Conclusions (20-30 words):**\n",
      "   - Since the program data is processed and used to control the device, it implies that the system can be effectively designed for both regulatory compliance and non-regulatory applications.\n",
      "   - The ability to monitor and analyze vibrations in mines or tunnels could have significant implications for safety, efficiency, and environmental protection.\n",
      "\n",
      "**Hypothesis (50 words):**\n",
      "\n",
      "A device for monitoring vibrations from controlled detonations in mines is advantageous as it allows the temporal elements of the vibrations to be studied. This information can be particularly beneficial when studying resultant vibrations relative to time.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Initialize the LLM client (ensure the host URL is correct)\n",
    "client = Client(host='http://jupyter.weburban.com:10434')\n",
    "\n",
    "# Function to load text files\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load example hypothesis, experiment, observation, evaluation, logical conclusions, and new document\n",
    "\n",
    "new_text_path = \"Data/device.txt\"\n",
    "\n",
    "\n",
    "new_text = load_text_file(new_text_path)\n",
    "\n",
    "# Split the new document into chunks using NLTK sentence tokenization\n",
    "nltk.download('punkt')\n",
    "chunks = sent_tokenize(new_text)\n",
    "\n",
    "# Initialize a BERT model for sentence embeddings\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Convert the document's sentences into embeddings\n",
    "sentence_embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "Extract relevant text for hypothesis, experiment, observation, evaluation, and logical conclusions extraction from the following document content.\n",
    "\n",
    "**Definition of Hypothesis:**\n",
    "A research hypothesis is a concise statement about the expected result of an experiment or project, often derived from prior research and observations.\n",
    "\n",
    "**Definition of Experiment:**\n",
    "An experiment to prove a hypothesis is a structured and controlled procedure designed to test whether a hypothesis is true or false. In an experiment, specific variables are manipulated, while others are kept constant, allowing researchers to observe and measure the effects of the changes.\n",
    "\n",
    "**Definition of Observation:**\n",
    "An observation refers to the data or information collected during the experiment. This includes qualitative or quantitative details noted directly from the experiment's outcome.\n",
    "\n",
    "**Definition of Evaluation:**\n",
    "Evaluation involves assessing the results of the experiment and determining whether they support the hypothesis, discussing possible implications and limitations.\n",
    "\n",
    "**Definition of Logical Conclusions:**\n",
    "Logical conclusions are the deductions made based on the evaluation and analysis of the experiment and observation, forming a final interpretation of the findings.\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query into an embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "# Compute similarity scores between the query and sentence embeddings\n",
    "similarity_scores = cosine_similarity([query_embedding], sentence_embeddings)[0]\n",
    "\n",
    "# Retrieve the top-k most relevant chunks (adjust k as needed)\n",
    "top_k = 10  # Number of chunks to retrieve\n",
    "top_k_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Retrieve the relevant chunks and add some context windows\n",
    "relevant_chunks = []\n",
    "for idx in top_k_indices:\n",
    "    start = max(0, idx - 1)  # Add previous sentence as context\n",
    "    end = min(len(chunks), idx + 2)  # Add next sentence as context\n",
    "    relevant_chunks.extend(chunks[start:end])\n",
    "\n",
    "# Combine relevant chunks into one text\n",
    "relevant_text = \" \".join(relevant_chunks)\n",
    "\n",
    "# Prepare the refined prompt with clearer instructions and chain-of-thought guidance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"\n",
    "    Based on the provided definitions, identify each element in the following **Relevant Text**: hypothesis, experiment, observation, evaluation, and logical conclusions.\n",
    "\n",
    "    **Example Definitions** (simplified):\n",
    "    - **Hypothesis**: Expected result or proposed relationship.\n",
    "    - **Experiment**: Steps to validate the hypothesis.\n",
    "    - **Observation**: Data gathered during the experiment.\n",
    "    - **Evaluation**: Analyzing results to validate the hypothesis.\n",
    "    - **Logical Conclusions**: Final interpretation based on evaluation.\n",
    "\n",
    "    **Relevant Text:**\n",
    "    {relevant_text}\n",
    "\n",
    "    **Instructions**:\n",
    "    1. Use only the information in the Relevant Text to create responses.\n",
    "    2. Each answer should be concise and based solely on the content provided.\n",
    "    3. Avoid mirroring any example phrases directly; describe observations, evaluations, and conclusions uniquely.\n",
    "    \n",
    "    **Hypothesis:**\n",
    "    - Identify the hypothesis in 1-2 concise sentences.\n",
    "    \n",
    "    **Experiment:**\n",
    "    - Provide a short description of the experiment procedure.\n",
    "    \n",
    "    **Observation:**\n",
    "    - Summarize key observations from the experiment.\n",
    "    \n",
    "    **Evaluation:**\n",
    "    - Summarize the evaluation, including support for the hypothesis.\n",
    "    \n",
    "    **Logical Conclusions:**\n",
    "    - State the final interpretation based on the evaluation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt with the relevant text\n",
    "filled_prompt = prompt_template.format(text=relevant_text)\n",
    "\n",
    "# Use the updated Ollama client to invoke the LLM\n",
    "response = client.chat(model='qwen2.5:0.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': filled_prompt,\n",
    "    }\n",
    "])\n",
    "\n",
    "# Print the generated hypothesis, experiment, observation, evaluation, and logical conclusions\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dbb39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
