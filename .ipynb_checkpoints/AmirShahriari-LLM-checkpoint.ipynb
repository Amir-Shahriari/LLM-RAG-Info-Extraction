{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00668345-a40a-46f9-8f04-6d8df65fb8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.11/site-packages (0.2.12)\n",
      "Collecting docx\n",
      "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (3.10.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.2.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.1.96)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Collecting lxml (from docx)\n",
      "  Downloading lxml-5.2.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting Pillow>=2.0 (from docx)\n",
      "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
      "Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.2.2-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docx\n",
      "  Building wheel for docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53894 sha256=96ba3aa50cd8a4b48617cecce36412335ae1935553b112e110828a27e3b1007e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
      "Successfully built docx\n",
      "Installing collected packages: Pillow, lxml, docx\n",
      "Successfully installed Pillow-10.4.0 docx-0.2.4 lxml-5.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dafb8bc-f0c7-46f0-9707-bcdac2f5e439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author, a welder, discusses his team's work on testing a new welding technique that uses dynamically controlled arc length. The technique is innovative and cutting-edge, with the potential to reduce defects in certain materials by around 62.5%. However, there are still some inconsistencies in the data which need to be ironed out before fully switching over to this method. Despite the promising results, the author notes that the technique is not ready for prime time yet and they need more data to say for sure. The author also mentions the potential of this technique for \"tough-to-weld\" materials like steel, which could potentially reduce heat input variations and improve overall output quality. Despite some methodological challenges, the author sees a lot of promise in this new welding technique.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\", \n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "def load_word_file(file_path):\n",
    "    import docx\n",
    "    doc = docx.Document(file_path)\n",
    "    content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Dummy.txt\")\n",
    "\n",
    "# Prepare the prompt with the document content\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Given the following document content: {document}\\n\\n{question}\",\n",
    "    input_variables=[\"document\", \"question\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with document content and a question\n",
    "filled_prompt = prompt_template.format(\n",
    "    document=document_content,\n",
    "    question=\"\"\"Mine for the following elements within the evidence:\n",
    "Hypothesis\n",
    "Experiment(s)\n",
    "Observation\n",
    "Evaluation\n",
    "Logical conclusions)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015976fb-39fe-4625-9290-d6e523de5eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In response to the given text material, here is a revised version with the format and conventions followed:\n",
      "\n",
      "Format:\n",
      "- Hypothesis: \"Wow, that's a big difference! We had to change our welding machines due to variations in the data.\"\n",
      "- Experiment: \"To avoid any biases, we randomized the order of joint welds to avoid any accidental variation. However, there might be some unintentional bias since alternating between Technique A and Technique B did not produce a perfect setup.\"\n",
      "- Observation: \"We didn't find a statistically significant difference in the defects between Technique A and Technique B. The new technique using dynamically controlled arc length seemed promising, but still needs to be fine-tuned before we switch over to full welding.\"\n",
      "- Evaluation: \"The new technique seems to reduce defects, but it's not quite ready for prime time yet due to inconsistent results.\"\n",
      "- Logical Conclusions: \"Our research suggests that the new welding technique using dynamically controlled arc length could potentially be useful for certain materials in the industry. However, more data is needed before a decision can be made on its application.\"\n",
      "\n",
      "The text material was originally written in a style that may not follow specific guidelines for academic writing or have specific conventions for formatting a response. The revised format follows these guidelines to ensure consistency and clarity of communication.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    base_url=\"http://notebooks.weburban.com:12434\",\n",
    "    model=\"tinyllama\"\n",
    ")\n",
    "\n",
    "# Load the text document\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Example usage: Load a text file\n",
    "document_content = load_text_file(\"Dummy.txt\")\n",
    "\n",
    "# Prepare the prompt with the document content\n",
    "prompt_template = PromptTemplate(\n",
    "    template=f\"\"\"Given the provided text:\n",
    "{document_content}\n",
    "\n",
    "Please extract and clearly label the following elements from the document content that is in the text:\n",
    "\n",
    "Format your response with each element on a new line, prefixed with the element name followed by a colon. follow the formatting below for your response\n",
    "\n",
    "formatting that you must follow:\n",
    "\n",
    "Hypothesis: [Text]\n",
    "\n",
    "Experiment(s): [Text]\n",
    "\n",
    "Observation: [Text]\n",
    "\n",
    "Evaluation: [Text]\n",
    "\n",
    "Logical Conclusions: [Text]\n",
    "\n",
    "\n",
    "keep that in mind your final response must follow the format above and have only Hypothesis, Experiment, Observation, Evaluation, Logical Conclusions. there must be the\n",
    "topics word at the start of each line\n",
    "\"\"\",\n",
    "    input_variables=[\"document\"]\n",
    ")\n",
    "\n",
    "# Fill the prompt template with document content\n",
    "filled_prompt = prompt_template.format(document=document_content)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ecff4b3-3ac6-4ad8-91fe-d8f87d1a1ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'BaseLLM', 'Callable', 'Dict', 'Type', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__getattr__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_import_ai21', '_import_aleph_alpha', '_import_amazon_api_gateway', '_import_anthropic', '_import_anyscale', '_import_aphrodite', '_import_arcee', '_import_aviary', '_import_azure_openai', '_import_azureml_endpoint', '_import_baichuan', '_import_baidu_qianfan_endpoint', '_import_bananadev', '_import_baseten', '_import_beam', '_import_bedrock', '_import_bigdlllm', '_import_bittensor', '_import_cerebriumai', '_import_chatglm', '_import_clarifai', '_import_cohere', '_import_ctransformers', '_import_ctranslate2', '_import_databricks', '_import_databricks_chat', '_import_deepinfra', '_import_deepsparse', '_import_edenai', '_import_fake', '_import_fireworks', '_import_forefrontai', '_import_friendli', '_import_gigachat', '_import_google_palm', '_import_gooseai', '_import_gpt4all', '_import_gradient_ai', '_import_huggingface_endpoint', '_import_huggingface_hub', '_import_huggingface_pipeline', '_import_huggingface_text_gen_inference', '_import_human', '_import_ipex_llm', '_import_javelin_ai_gateway', '_import_koboldai', '_import_konko', '_import_llamacpp', '_import_llamafile', '_import_manifest', '_import_minimax', '_import_mlflow', '_import_mlflow_ai_gateway', '_import_mlflow_chat', '_import_mlx_pipeline', '_import_modal', '_import_mosaicml', '_import_nlpcloud', '_import_oci_gen_ai', '_import_oci_md_tgi', '_import_oci_md_vllm', '_import_octoai_endpoint', '_import_ollama', '_import_opaqueprompts', '_import_openai', '_import_openai_chat', '_import_openllm', '_import_openlm', '_import_pai_eas_endpoint', '_import_petals', '_import_pipelineai', '_import_predibase', '_import_predictionguard', '_import_promptlayer', '_import_promptlayer_chat', '_import_replicate', '_import_rwkv', '_import_sagemaker_endpoint', '_import_sambastudio', '_import_sambaverse', '_import_self_hosted', '_import_self_hosted_hugging_face', '_import_sparkllm', '_import_stochasticai', '_import_symblai_nebula', '_import_textgen', '_import_titan_takeoff', '_import_titan_takeoff_pro', '_import_together', '_import_tongyi', '_import_vertex', '_import_vertex_model_garden', '_import_vllm', '_import_vllm_openai', '_import_volcengine_maas', '_import_watsonxllm', '_import_weight_only_quantization', '_import_writer', '_import_xinference', '_import_yandex_gpt', '_import_yi', '_import_you', '_import_yuan2', 'get_type_to_cls_dict', 'ollama', 'warn_deprecated']\n"
     ]
    }
   ],
   "source": [
    "import langchain_community.llms as llms\n",
    "\n",
    "# Check available models in the module\n",
    "print(dir(llms))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
